<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="PW&#39;s notes">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="PW&#39;s notes">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PW&#39;s notes">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/4/"/>





  <title>PW's notes</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PW's notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/15/Introduction to Restricted Boltzmann Machines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/15/Introduction to Restricted Boltzmann Machines/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-15T15:52:57+08:00">
                2018-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction-to-Restricted-Boltzmann-Machines"><a href="#Introduction-to-Restricted-Boltzmann-Machines" class="headerlink" title="Introduction to Restricted Boltzmann Machines"></a>Introduction to Restricted Boltzmann Machines</h1><p>#推荐引擎/论文</p>
<p>限制玻尔兹曼机器本质上是执行因子分析(SVD++)的二进制版本。 （这是思考RBM的一种方式;当然还有其他方法，还有很多不同的方法来使用RBM，但是我将在这篇文章中采用这种方法）。不去预测用户对物品的评分,而是只是告诉你他们是否喜欢电影，而RBM将尝试发现可以解释这些电影选择<strong>激活(被喜欢)的潜在因素</strong>。</p>
<p>从技术上讲，限制玻耳兹曼机是一个随机神经网络（<strong>随机意味着这些激活有一个概率因素</strong>).</p>
<ul>
<li>One layer of visible units (用户对电影的preferences,这是已知的)</li>
<li>One layer of hidden units (the latent factors we try to learn)</li>
<li>A bias unit(is a way of <strong>adjusting for the different inherent popularities of each movie</strong>)</li>
</ul>
<p>可视层和隐含层间元素是全相连的(反之也是), bias单元连接到所有可视单元和所有hidden单元.简单情况下,相同层间的元素就不相连.</p>
<p>例如，假设我们有一组六部电影（哈利·波特，阿凡达，LOTR 3，角斗士，泰坦尼克号和闪光），我们要求用户告诉我们他们想看哪些。 如果我们想要学习两个潜在的电影偏好unit - 例如，我们六套电影中的两个自然组合看起来像<strong>幻想</strong>（包含哈利波特，阿凡达和LOTR 3）和<strong>奥斯卡获奖者</strong>（包含LOTR 3，角斗士 ，泰坦尼克号），所以我们可能希望我们的潜在单位符合这些类别, 那么其RBM就像下面所示.<br><img src="/2018/01/15/Introduction to Restricted Boltzmann Machines/DB8F97A3-5350-4488-BCCB-4DEC4F4E768D.png" alt=""></p>
<h2 id="State-Activation"><a href="#State-Activation" class="headerlink" title="State Activation"></a>State Activation</h2><p>让我们看看在知道权值的情况下,如何计算unit i的激活状态.</p>
<ol>
<li>计算激活状态<img src="/2018/01/15/Introduction to Restricted Boltzmann Machines/D1E0CAB3-6F14-4EA1-AA36-47A991DC0727.png" alt="">,xj就是0或者1(相当于aggregateMessages)</li>
<li>令p(i)=sigmoid(αi),于是你又可以把pi看做概率</li>
<li>我们有p(i)概率turn on unit i, (1-p)的概率turn off it.(因为现实中你并不能依次完全推定)</li>
<li>解释一下就是,units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states</li>
</ol>
<p>由例子看的是,隐含节点表示了<em>fantasy</em>或者<em>Oscar winners</em>,那么:</p>
<ul>
<li>If Alice has told us her six binary preferences on our set of movies, six movies send messages to the hidden units,telling them to update themselves. </li>
<li>同样的, if we know that one person likes SF/fantasy)(对应的隐因子turn on了),然后我们就可以ask the RBM which of the movie units that hidden unit turns on. 然后 the hidden units send messages to the movie units, telling them to update their states,然后那些fantasy电影就最可以被turn on(推荐)了…</li>
</ul>
<h2 id="学习权值"><a href="#学习权值" class="headerlink" title="学习权值"></a>学习权值</h2><ol>
<li>首先有随机的权值参数什么的…. 取一个data</li>
<li>从这个data上计算隐因子的<img src="/2018/01/15/Introduction to Restricted Boltzmann Machines/E58CD472-FC2D-4A8B-BD6B-8500B48FDA9F.png" alt="">,然后把隐因子的输出xj,按sigmoid(aj)的概率来设置.</li>
<li>然后对每一条边eij,计算Positive(eij)=xi*xj…(看他们是不是都是turn on的)</li>
<li>重建visible unit, 就是计算其ai,然后按概率计算其输出xi,然后利用xi再次计算隐因子…然后现在对每条边再计算Negative(eij)=xi*xj</li>
<li>更新边eij的权值:<br><img src="/2018/01/15/Introduction to Restricted Boltzmann Machines/6D463E8D-FE06-40A6-A22B-9D0C20F51BBA.png" alt="">,L是学习率</li>
<li>对所有条data训练上面的过程…..</li>
<li>重复1~6,直到(error between the training examples and their reconstructions falls below some threshold)</li>
</ol>
<p>它的意义是什么呢?</p>
<ol>
<li>In the first phase, <strong>Positive(eij) measures the association between the ith and jth unit</strong> that we want the network to learn from our training examples;</li>
<li>重建阶段,RBM根据对hidden unit的假设生成visible unit的状态,Negative measure了当没有unit fixed to训练数据时,网络本身产生的关联（或“白日梦”）。</li>
<li>反正这就叫什么<strong>对比散度</strong>咯</li>
</ol>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul>
<li>Above, Negative(eij)Negative(eij) was determined by taking the product of the iith and jjth units after reconstructing the visible units once and then updating the hidden units again. We could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network’s daydreams more accurately.</li>
<li>Instead of using Positive(eij)=xi∗xjPositive(eij)=xi∗xj, where xixi and xjxj are binary 0 or 1 states, we could also let xixi and/or xjxj be activation probabilities. Similarly for Negative(eij)Negative(eij).</li>
<li>We could penalize larger edge weights, in order to get a sparser or more regularized model.</li>
<li>When updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., L∗(Positive(eij)−Negative(eij)L∗(Positive(eij)−Negative(eij)) and the step previously taken.<br>I* nstead of using only one training example in each epoch, we could use batches of examples in each epoch, and only update the network’s weights after passing through all the examples in the batch. This can speed up the learning by taking advantage of fast matrix-multiplication algorithms.</li>
</ul>
<h2 id="…"><a href="#…" class="headerlink" title="…"></a>…</h2><p>这个玩意,你把hidden unit的个数小于visible unit的个数,映射之后不就相当于做了数据降维,特征提取嘛..这也跟SVD是一个意思…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/15/一些网上的调研/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/15/一些网上的调研/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-15T15:26:40+08:00">
                2018-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一些网上的调研"><a href="#一些网上的调研" class="headerlink" title="一些网上的调研"></a>一些网上的调研</h1><p>#推荐引擎/论文</p>
<p>传统的协同过滤、tag、和各种你能看懂的基于规则或者群体智能甚至物理学的传统推荐方法在逐渐被淘汰。这点在内容推荐上尤其明显，因为这些传统推荐策略都<strong>无法解决对长尾内容的精细化个性化推荐和热度穿透等问题</strong>，而且效果上也远远不如机器学习方法。当然，这些传统的启发式策略在做冷热启动、降级策略、初筛策略时仍然是有价值的.</p>
<p>做法就是CTR.  基于机器学习的推荐系统说白了和计算广告是类似的，就是用超大规模的稀疏表达的特征（上亿轻轻松松），和巨量的样本，训练一个预估用户点击率、浏览时长、点赞率、购买率（反正是你的某个业务目标）的模型。可以当做feature的新特征包括</p>
<ul>
<li>GPS坐标、ip地址、屏幕分辨率</li>
<li>直接对内容的分词</li>
<li>直接对图片、视频、音频信号的编码当做特征</li>
<li><strong>实时session</strong>：用户最近的行为，直接拿来做特征，可以让模型具有<strong>极好的个性化效果</strong>，前提是你会用正确的方法来使用</li>
<li>使用GBDT构造特征，到使用某些奇淫技巧把<strong>超级稀疏的特征以类似embedding的方式变成连续特征直接用上GBDT</strong>….</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/12/FFM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/12/FFM/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-12T18:43:01+08:00">
                2018-01-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><p>#推荐引擎/论文</p>
<p>优秀在<strong>大数据</strong>以及<strong>特征稀疏</strong>的场景.旨在<strong>解决稀疏数据下的特征组合问题</strong></p>
<p>当前比较流行的模型有:</p>
<ol>
<li>人工特征工程 + LR(Logistic Regression)、</li>
<li>GBDT(Gradient Boosting Decision Tree) + LR[1][2][3]、</li>
<li>FM（Factorization Machine）[2][7]</li>
<li>FFM（Field-aware Factorization Machine）[9]模型</li>
</ol>
<p>首先,特征向量构建过程中使用<strong>one-hot</strong>导致特征稀疏,这使得特征和label的相关性不高.而可以发现的是,<strong>某些特征经过关联之后，与label之间的相关性就会提高</strong>。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。也就是说,<strong>关联特征与lebel具有正相关性.</strong></p>
<p>所以在线性模型(1)的基础上,加入<strong>组合特征</strong>,于是公式变为(2),当wij&gt;0则组合特征与label正相关,反之负相关.模型的参数是w0,多个wi,多个wij.</p>
<blockquote>
<p><img src="/2018/01/12/FFM/4BCFD27D-B369-46AA-9E7D-64DDB36DB307.png" alt="">……(1)<br><img src="/2018/01/12/FFM/0A507467-4BBE-4AEF-A0C7-D9BA31187832.png" alt="">……(2)  </p>
</blockquote>
<p>然而,训练(2)并不是那么简单,因为数据稀疏性普遍存在,你得当xi,xj都不为0都时候才提供了有效的训练数据,以探究(训练)xi与xj的关联关系(权值)wij. 那么,怎么办呢?</p>
<p>你可以把所有参数wij组成一个矩阵W,且W是实对称矩阵. 根据数学理论,实对称矩阵就能分解为(3). 那么注意到,V的第j列就是j号特征的隐向量,于是根据矩阵运算,wij就等于<strong>Vi·Vj</strong>,即v的第i列与第j列的点积.</p>
<blockquote>
<p><img src="/2018/01/12/FFM/E167C5E0-92FB-4376-BEC2-03D9004BDD2E.png" alt="">……..(3)<br>于是,(2)式变化为(4)式<br><img src="/2018/01/12/FFM/8D3555F0-EED3-4E7E-9977-BC27B60AE536.png" alt="">……(4)  </p>
</blockquote>
<p>隐向量的长度为k. 根据(4),我们只需要解n(特征维度)个k维向量,共kn个二次项参数.而对二而言,这个数是n(n-1)/2个.于是二次项系数大大减少.并且xhxi的稀疏和xixj的稀疏也不再是相互独立的了,于是训练数据可以相互影响隐因子..因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数.</p>
<p>意思就是,所有xi非0的数据都可以训练Vi,进而训练xhxi和xjxj乃至更多组合特征的系数.</p>
<p>显而易见，(4)是一个<strong>通用的拟合方程</strong>，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用<strong>MSE</strong>（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge/Cross-Entropy损失来求解分类问题。</p>
<p>当然,这也和线性模型一样,是一个回归的模型,如果要用于二分类,你还得把FM的输出z做一个sigmoid变换到y….</p>
<p>解的过程中,(4)的组合项可以化简为:,FM的二次项可以化简，其复杂度可以优化到 <strong>O(kn)</strong></p>
<blockquote>
<p><img src="/2018/01/12/FFM/BB03726C-046A-4E39-A279-05D20820F3E7.png" alt="">…..(5)  </p>
</blockquote>
<p>利用随机梯度下降计算预测值对各个参数的梯度,是…<br><img src="/2018/01/12/FFM/830499CB-EEE6-48A2-BF19-9864DBC61660.png" alt="">……(6)</p>
<p>其中，vj,fvj,f 是隐向量 vjvj 的第 ff 个元素。由于 ∑nj=1vj,fxj∑j=1nvj,fxj 只与 ff 有关，而与 ii 无关，在每次迭代过程中，只需计算一次所有 ff 的 ∑nj=1vj,fxj∑j=1nvj,fxj，就能够方便地得到所有 vi,fvi,f 的梯度。显然，计算所有 ff 的 ∑nj=1vj,fxj∑j=1nvj,fxj 的复杂度是 O(kn)O(kn)；已知 ∑nj=1vj,fxj∑j=1nvj,fxj 时，计算每个参数梯度的复杂度是 O(1)O(1)；得到梯度后，更新每个参数的复杂度是 O(1)O(1)；模型参数一共有 nk+n+1nk+n+1 个。因此，FM参数训练的复杂度也是 O(kn)O(kn)。综上可知，FM<strong>可以在线性时间训练和预测</strong>，是一种非常高效的模型。</p>
<p>FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等[7]。…</p>
<p>相比于推荐系统中矩阵分解模型,MF只有两类特征 u 和 i 的FM模型。对于FM而言，我们可以<strong>加任意多的特征</strong>，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM，在此不再赘述。</p>
<h2 id="FFM-1"><a href="#FFM-1" class="headerlink" title="FFM"></a>FFM</h2><p>通过引入field的概念，FFM把相同性质的特征归于同一个field。以上面的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。也就是把one-hot里的<strong>各个枚举值的集合</strong>当做一个Field. </p>
<p>其公式变化为:</p>
<blockquote>
<p><img src="/2018/01/12/FFM/8EF6162D-D0E3-4DB2-B3B6-1869355876FB.png" alt="">…….(6)  </p>
</blockquote>
<p>可以看出,隐向量Vi变为了隐向量V(i,fj),解释如下:</p>
<p>在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量V(i,fj)。 于是,本来对特征xi,其有一个本身隐向量Vi,但是现在,他有<strong>filed个数</strong>个隐向量,表示xi基于filed j的隐向量V(i,fj)…</p>
<p>举个例子,也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候<strong>使用不同的隐向量(V(day,f(ad)和V(day,f(country))</strong>，<strong>就看和谁作组合特征就用谁那个field的隐向量</strong>这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 nn 个特征属于 ff 个field，那么FFM的二次项有 nfnf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。<strong>FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型</strong>。</p>
<p>设隐向量长k,那么FM中二次项参数共nk,而FFM中是nfk…此外,由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 <strong>O(kn2)</strong>。</p>
<p>求解和优化这里就不详细记录了,可以参考<a href="https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团FFM</a>.下面再形象地给一个例子.<br><img src="/2018/01/12/FFM/08ED3848-0AAD-459D-BCCB-988CA8F827AF.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/12/A study of the dynamic features of recommender systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/12/A study of the dynamic features of recommender systems/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-12T15:47:29+08:00">
                2018-01-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="A-study-of-the-dynamic-features-of-recommender-systems"><a href="#A-study-of-the-dynamic-features-of-recommender-systems" class="headerlink" title="A study of the dynamic features of recommender systems"></a>A study of the dynamic features of recommender systems</h1><p>#推荐引擎/论文</p>
<p>研究的主要是<strong>changing needs of user requirements as well as changes in the systems’ contents(各种变化的特征)</strong></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This can be difﬁcult at times when <strong>little user data is available</strong> for inferring the user evolving needs as well as <strong>system keeps on evolving with time</strong>.静态用户配置文件无法判断用户在一段时间内的偏好，而这是目前推荐系统中普遍流行的方法.</p>
<p>现有的动态从两方面下手:the temporal evolution is presented in terms of <strong>user preferences that evolve with time</strong> or the <strong>item contents that gets changed due to addition of new items or deletion of older items</strong>.而实际上,演化的问题还有多个因素,这让问题更复杂.因此，有必要将现有的二维框架扩展到多维因子分析模型（Adomavicius and Tuzhilin，2001）.</p>
<p>在本文中，我们通过提出一个称为DRS的新的RS类别来描述RS中的动态概念.</p>
<h2 id="Dynamic-recommender-systems"><a href="#Dynamic-recommender-systems" class="headerlink" title="Dynamic recommender systems"></a>Dynamic recommender systems</h2><p>动态RS是各种参数的组合,每个参数都可是开发独立系统的核心要素.DRS,可以关注用户面,系统面或者别的层面的(隐式,显式)变化,并调整推荐结果.</p>
<p>把做推荐看做去找人咨询,那么这个被咨询人可能得懂心理学.人类的思想是非常复杂和难以解释的; 仍然有各种各样的<strong>人格属性</strong>，可以帮助揭示人类行为,然后可以在推荐系统中实施.  一种使推荐过程更加透明以及与现实生活建议类似的新颖方法应该总是考虑心理因素，如<strong>用户的信任和社交网络</strong></p>
<p>系统方面也注册了一些内容方面的变化，经历了更新和演变的阶段。 这些变化也会影响提供给用户的推荐类型，并可以在RS中实现（Lathia et al。2009）</p>
<p>然而，推荐系统行为的变化超越了时间因素，涉及到情境，新颖性，偶然性，实时动态性以及多样性。 这些<strong>参数</strong>中的每一个都有助于DRS的动态行为。 因为<strong>参与处理上述任何一个参数的推荐系统被称为DRS</strong>。</p>
<h2 id="Classiﬁcation-of-dynamic-recommender-systems"><a href="#Classiﬁcation-of-dynamic-recommender-systems" class="headerlink" title="Classiﬁcation of dynamic recommender systems"></a>Classiﬁcation of dynamic recommender systems</h2><p>动态推荐系统还没有被建立为推荐系统研究的一个独立的领域。因此，这一领域的贡献分散在许多不同的领域，不同的问题正在被分类. 当对传统方法进行重新训练无法跟上用户偏好的变化速度时，时间维度和其他动态因素的包含变得重要. 在本文中,<strong>给出了一个全新的推荐系统分类</strong>，它们是动态的，并解决了这个问题。这个新的分类方案被用来解释DRSs的不同方面。</p>
<p>分类所依据的参数迎合了推荐系统的动态特性。这些参数涵盖了研究领域中处理的推荐系统的各个维度，并且与整个系统在整个网络上的演变相互联系. 我们所确定的参数是:</p>
<ol>
<li>temporal context</li>
<li>novelty</li>
<li>serendipity惊喜性</li>
<li>diversity多样性</li>
<li>动态环境</li>
</ol>
<h3 id="Temporal-changes"><a href="#Temporal-changes" class="headerlink" title="Temporal changes"></a>Temporal changes</h3><p>Koren在他的研究中强调了RS（Koren 2009）中时间动态的影响。他强调需要在RS中包括时间变化来提高建议的准确性。他还提出了一个矩阵分解模型，它跟踪数据的整个寿命周期中的时间变化行为，从而利用所有相关的组件.</p>
<p>Lathia等。 （2009）提供了一个不同的视角,这是系统的方法,他研究了<strong>每周重新训练CF算法</strong>的效果作为时间依赖预测问题，<strong>该技术基于到当前时间测量的性能临时调整KNN邻域的大小</strong>。(<strong>我们确实每周可以训练,这点可以考虑进来</strong>)</p>
<p>如果在ctr重排序的过程中采取<strong>动态相关的特征??</strong></p>
<p>Nathanson et al. (2007) that considers changes in user’s preferences with time when selecting the next item to recommend, r<strong>eordering them based on the most recent rating.</strong></p>
<p> Lee and Park (2008). They combined the two dimensions involving <strong>product launch time</strong> , <strong>based on rating time</strong></p>
<p>Chu 和Park提出了a feature-based machine learning approach to personalize recommendation of new items,This approach maintains proﬁles of content of interest and updates their temporal characteristics…..比如物品的流行度和新鲜度…</p>
<p>Min and Han suggested a methodology that works at different stages of recommendation process for detecting user’s time-variant pattern in order to improve its performance.</p>
<p> De Pessemier et al. (2010) presents an empirical evidence that <strong>older consumption data has a negative inﬂuence on the recommendation accuracy</strong> in case of consumer centric RS.</p>
<h3 id="实时动态"><a href="#实时动态" class="headerlink" title="实时动态"></a>实时动态</h3><p>用户的选择依赖于许多容易随时变化的因素。因此，在线处理期间考虑到这种动态是必要的.</p>
<p>Baraglia et al. (2004) implemented a RS that <strong>collapse the two phases (online and offline) into a single online phase</strong>. This component dynamically creates links to pages that are not yet visited by a user and might be of his potential interest.</p>
<p>Chen和Han（2007）指出，在客户端创建私有动态用户配置文件（DUP）可以满足RS的隐私和准确性要求。他实施了一种名为CRESDUP的方法，收集，挖掘，发现商店并在客户端更新私人DUP.</p>
<p>Chandramouli et al. (2011) emphasis the need of real time recommender system in the era of social networking where <strong>user is continuously updating his proﬁle by adding new items</strong> (e.g., newsposts,Facebookpostings). 因此，他们提出了StreamRec，这是一种推荐系统架构，它利用了将推荐系统建模为复杂事件处理（CEP）应用程序的流处理系统</p>
<h3 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h3><p>许多研究人员正在阐述提供建议的context的重要性。诸如一天中的时间，夏季或冬季的各种背景特征，无论用户在寻找自己还是其他人在很大程度上影响用户的决定。</p>
<p>. Gonzalez et al. (2007) provides a novel perspective to improve the performance of RS through embedding emotional context. 这种情绪信息是以增量方式获得的，以丰富日常生活中的建议。</p>
<p>Woerndl和Groh（2007）描述了通过使用社交网络和<strong>用户的物理位置</strong>来处理上下文的不同方法。</p>
<p><strong>所以,可以在呼叫中心推荐中考虑上下文信息,我觉得这是很有价值的</strong></p>
<p>Said（2010）通过构建用于电影的上下文感知RS的概念架构来处理上下文识别的问题。 而且，<strong>Jancsary等人 （2010）假定用户对特定项目的偏好不仅取决于主题和命题内容，还取决于用户的当前上下文。</strong> 基于他的假设，他基于现实生活点击流和发布数据对上下文和非命题特征的优点进行了系统的评估。 在另一种方法中，Baltrunas等人 （2012）也提出了一个基于上下文的推荐系统，他们最近模拟了情境状况以收集这些特征对用户评分的影响。 根据收集到的数据以及建立系统的相关背景特征的重要性。</p>
<p> Kahng等人 （2011）提出了一种新的方法，<strong>其将多个上下文特征并入到排名模型中</strong>，并且根据他们的排名赋予更多的权重。</p>
<h3 id="Diversity"><a href="#Diversity" class="headerlink" title="Diversity"></a>Diversity</h3><p>多样性需要向用户呈现不同类型的推荐.虽然多样性的特征与准确性形成了鲜明的对比，但是许多研究者试图将两者协调起来.</p>
<p><strong>breaking the barrier of similarity</strong></p>
<p>齐格勒等。 （2005）提出了主题多样化，这是一种新颖的方法，旨在平衡和多样化个性化推荐列表，以反映用户的全部兴趣范围。这个程序对准确性有一定的不利影响.</p>
<p>Kwon（2008）: 可以通过考虑rating variance来改善Top N项目选择的多样性,这可以与现有推荐技术结合使用的.此外，在这种方法中，用户可以通过一种adjusted raking and filtering combined approaches来控制推荐的准确性和多样性之间的平衡，并且根据用户筛选调整选择N个项目的条件的组合方法。</p>
<p>另一位研究人员（Zhang和Hurley 2008; Zhang 2009）开发了一个模型来<strong>最大化检索列表的多样性</strong>，同时保持与用户查询足够的相似性作为<strong>二进制优化问题</strong>。 还提出了一个新的项目新颖性评估指标来比较结果。</p>
<p> 最后，Lathia等。 （2010）也表明时间多样性是RS通过用户研究的一个重要方面。 他们还通过<strong>定义多样性指标来检验三个CF的多样性</strong>。 此外，它们提供了几种可用于改进建议多样性的方法。</p>
<h3 id="Novelty"><a href="#Novelty" class="headerlink" title="Novelty"></a>Novelty</h3><p>经过一段时间后，那就通用流行的东西一再被推荐.用户想看点新东西.(这在<strong>呼叫中心并不是问题</strong>)</p>
<p>Celma和Herrera（2008）提出了两种方法，以项目和以用户为中心来<strong>评估新颖推荐的质量</strong>。他们观察到，尽管CF比CBF推荐更加不novelty,但用户的感知质量更高。这是因为CF是偏向流行，影响新颖性和网络拓扑，而CBF根本不受影响。</p>
<p>朴智林（2008）以全新的方式处理新奇的概念。他们试图研究推荐系统的<strong>长尾问题</strong>，其中长尾的许多项目只有少量的评级，因此很难在推荐系统中使用它们。他们很少被推荐，但是也有能力吸引用户…</p>
<p>Abbassi等人 （2009）检查了推荐系统中过度专业化的情况，这是因为返回的项目与用户以前评估过的项目过于相似。 他们开发了一种<strong>算法之外的算法（OTB），试图识别用户曝光不足的区域，通过冒险帮助用户发现新的信息，同时保持高度的相关性</strong></p>
<h3 id="Serendipity"><a href="#Serendipity" class="headerlink" title="Serendipity"></a>Serendipity</h3><p>偶然性是在<strong>寻找不相关的东西</strong>的同时，发现幸运的发现的倾向.</p>
<p>Oku and Hattori (2011) propose a Fusion-based Recommender System which <strong>ﬁnds new serendipitous items that have mixed features of two user-input items,</strong> produced by mixing the two items together.</p>
<h3 id="大表格"><a href="#大表格" class="headerlink" title="大表格"></a>大表格</h3><p><img src="/2018/01/12/A study of the dynamic features of recommender systems/AB5FEFC8-8BC5-45CA-85ED-E4378B8B5B94.png" alt=""></p>
<ul>
<li>Burke2010的论文说的就是制定评估指标来评估推荐过程的动态性….</li>
<li>BUrke2007说的是伯克对混合推荐系统进行了详细的调查，他比较了这种系统发展的不同策.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/04/优化Spark的笛卡尔积/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/04/优化Spark的笛卡尔积/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-04T21:29:56+08:00">
                2018-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="优化Spark的笛卡尔积"><a href="#优化Spark的笛卡尔积" class="headerlink" title="优化Spark的笛卡尔积"></a>优化Spark的笛卡尔积</h1><p>#spark</p>
<p><img src="/2018/01/04/优化Spark的笛卡尔积/2A95D681-C508-4072-8549-B313E268D73A.png" alt=""></p>
<p><img src="/2018/01/04/优化Spark的笛卡尔积/FC6A0B5A-EC6F-42A4-869E-B7982AA04AE4.png" alt=""></p>
<p>mappartititions和map效果其实是一样的 就是map返回一个元素 但是mapprtition返回一个元素列表 然后内部再展开到map一样的形式<br>所以我们如果用map 那就是得到一个个（用户 物品与相似对）然后再groupbykey 如果用mappartition那就是得到list（用户 物品与相似对) 然后内部会展开 然后我们还是groupbykey 操作上并没啥不同 </p>
<p>还有一个考虑 因为我们采用用户 物品与相似对的格式 所以哪怕物品rdd大一些 但是若是把其广播出去 那groupbykey的时候应该就不用网络数据传输 我感觉会快一些 验证一下</p>
<p>或者说我们可以看看Als是怎么实现的</p>
<p>或者用dateframe api<br>特征数据转换为11个row 然后cross join<br> 然后withcolumn创建新row也就是内积然后只留下内积row<br>Topn的时候？</p>
<p>明天的计划步骤是</p>
<ol>
<li>先重跑一下 输出一下两个rdd分区数 使用psrtitions.size或者numpzrtitions看</li>
<li>测试广播用户 编码的时候先跑一个对两个pairrdd进行了 coalesce版本的代码看会不会小于14分钟</li>
<li>测试广播物品</li>
<li>若性能明显提升则算了</li>
<li>若没提升则使用dateframe api 注意crossjoin和喜爱的问题topn 需要配置spark.sql.crossJoin.enabled</li>
</ol>
<p>对于看分区 还可以这样看<br><img src="/2018/01/04/优化Spark的笛卡尔积/IMG_2442.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/25/内部类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/25/内部类/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-25T20:51:49+08:00">
                2017-12-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h1><p>#Java</p>
<ul>
<li><p>被匿名内部类访问的局部变量必须用final修饰,或者不用,但是必须不能再赋值,这个规则叫”effective final“.<br><img src="/2017/12/25/内部类/B68E9D99-DAB8-4755-9172-826F03833FBA.png" alt=""></p>
</li>
<li><p>静态内部类和非静态内部类就看做成员变量和类变量就好了,于是可以推出</p>
<ul>
<li>非晶态内部类中不能有static变量,static的任何东西</li>
<li>非静态内部类可以访问外部类的实例成员变量,但是外部类不能访问内部类实例成员变量</li>
<li>静态内部类不能访问外部类实例成员变量,只能访问类变量,这也是一个原则”静态的不能访问非静态的”</li>
<li>静态内部类还是可以有实例成员的,外部类依然不能访问</li>
</ul>
</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p><img src="/2017/12/25/内部类/777B9897-C468-433F-AF3E-8FC518D06BE3.png" alt=""><br><img src="/2017/12/25/内部类/B68FE6BB-43F6-44E5-A645-01771B6F373A.png" alt=""><br>要创外部类对象<br><img src="/2017/12/25/内部类/71F5F1A4-127C-4DED-B89B-23034ED33B4D.png" alt=""><br>不用创外部类对象</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/11/数据科学实战/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/11/数据科学实战/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-11T10:48:18+08:00">
                2017-12-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据科学实战"><a href="#数据科学实战" class="headerlink" title="数据科学实战"></a>数据科学实战</h1><p>#推荐引擎/知识</p>
<ol>
<li>CS</li>
<li>数学</li>
<li>统计学</li>
<li>Machine Learning.</li>
</ol>
<h3 id="推荐引擎"><a href="#推荐引擎" class="headerlink" title="推荐引擎"></a>推荐引擎</h3><ul>
<li>当不使用行为而使用某些profile对用户建模的时候,特征变量的选择变得很重要.可能对有些商品,有些场景,年龄特征不重要,有时候又至关重要,所以要根据研究对象的不同<strong>给不同的变量赋予不同的权重</strong>,确定权重可以看看变量之间的<strong>协方差矩阵</strong></li>
<li>使用KNN要防止过拟合,比如取k=1,那么唯一的邻居可能有很多噪声信息.</li>
<li>根据实践,采用精简的模型,比如年龄大的人通常性格比较保守,所以两个特征就拿一个就好,不然某项信息会过度使用,带来欠佳的模型表现.</li>
<li>兴趣的<strong>时变!</strong>我觉得这个很重要…..</li>
</ul>
<p>使用<strong>线性回归</strong>分类的思想是:<br>首先把问题定义为预测用户是否喜欢一个物品,每个物品都会有一个预测模型;假设每个用户都有三种属性变量,f1i,f2i,f3i.,预测用户i对每个商品的喜好可以使用</p>
<blockquote>
<p>pi=β1f1i+β2f2i+β3f3i+e.<br>这样的好处是它是天生的权重模型,解决了变量需要采用不同权重的问题.<br>坏处是,它预测的是单个商品,于是有多少商品就有多少个p模型,而且他没有考虑商品之间的相关关系,丢掉了太多信息.而且容易产生<strong>过拟合</strong>特别是当该商品行为数据不足的时候(参数β值过大).<br>为了解决过拟合,可以设定贝叶斯先验信息的方式强制所有参数估计值落在一定范围内,惩罚过大的参数值.这是思想,惩罚大参数的具体做法是<a href="http://mathbabe.org/2013/02/24/theoverburdened-prior/" target="_blank" rel="noopener">paper</a>.它会需要一个惩罚参数λ,这个参数可以通过模型交叉验证的方法,不停地训练和验证看效果,直到找到一个比较合理的值.</p>
</blockquote>
<h3 id="三大基本算法"><a href="#三大基本算法" class="headerlink" title="三大基本算法"></a>三大基本算法</h3><p>这三个算法涵盖了预测,分类,聚类.</p>
<h5 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h5><p>想要预测两个变量之间的(线性)数学关系就可以使用线性回归.由于只要函数是连续的,函数是可以被一些局部线性函数拟合的,所以<strong>局部线性关系</strong>假设在大多数情况都是合理的.</p>
<blockquote>
<p>模型对于数据来说,需要捕捉的两方面,<strong>趋势</strong>,和<strong>变动幅度(variation)</strong>  </p>
</blockquote>
<p>把模型表示为<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.40.51.png" alt="">两个参数就是截距和斜率.<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.41.21.png" alt=""><br>基本的线性回归自然使用最小二乘法最小化<strong>离差和平方</strong>.∑(真实值yi-预测值βxi)^2</p>
<p>当我们得到了模型,我们又有多大程度能信这个模型给出的结果呢?需要<strong>confidence</strong>.<br>首先我们可以把模型改进为<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.50.17.png" alt=""><br>e代表真实值与预测值的偏差,我们一般可以假设e服从正态分布N(0,o^2).于是在给定x的条件下y的真实值是一个条件概率:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.51.37.png" alt=""><br>如何估计e?其实你得到模型了以后,在测试集上就能测出一些个e来,即y-yi.那么e=<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.52.50.png" alt=""><br>于是o^2的<strong>无偏估计量</strong>为:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2014.53.17.png" alt=""><br>这个,也就叫<strong>均方误差MSE.</strong>,就是一种<strong>cost function损失函数</strong>.<br>关于<strong>置信度验证</strong>,可以用R^2,p值,但是只记一个交叉验证.分测试集和验证集,得到验证集上MSE误差最小的就是好的.</p>
<p>另外,即使画出图,也可以发现y和某些feature不是线性关系,比如你就觉得他是个二次关系,那么你就可以变化,以x^2作为z来当做新的变量.这时,依然可以是用线性回归拟合模型</p>
<blockquote>
<p>y=β0+β1x+β2(x^2)….  </p>
</blockquote>
<h5 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h5><p>这很简单,只要把新样本去跟旧样本计算距离,取k个邻居,看看k个邻居的标签,次数更多的标签值就是新样本的分类.k的选择通过交叉验证探究探究…<br>明显,这个模型不需要任何参数,对数据的分布也没有任何假设,但是还是有一些隐含的假设:</p>
<ol>
<li>在数据的特征空间中可以定义某种意义的“距离”。</li>
<li>有监督</li>
<li>我们选择和观测的<strong>特征变量</strong>对于预测因变量的标签值有所帮助，</li>
</ol>
<h5 id="Kmeans"><a href="#Kmeans" class="headerlink" title="Kmeans"></a>Kmeans</h5><p>这玩意儿关键是你得自己选择k(分类数).简单复习一下,是下面四步</p>
<ol>
<li>选择k个中心点来初始化,让中心点尽量靠近数据点,各个中心点之间要明显不同.</li>
<li>数据划分到最近的中心点</li>
<li>重新计算中心点.(mean最小)</li>
<li>重复,直至中心点不太变化.<blockquote>
<p>这是第(3)步的计算公式<img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.15.02.png" alt="">,即质心向量每个维度上的取值是所有该分类点在每个维度上取值的均值.  </p>
</blockquote>
</li>
</ol>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>首先介绍一下应用背景:用户会点击网页,最终形成一个行为矩阵,行是用户,列是网站.这同样是一个稀疏矩阵,就当做特征矩阵吧.而任务是分析<strong>广告</strong>是否会被点击,任务是<strong>某个用户点击我广告的概率</strong>比如下面的数据<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-12%2015.06.45.png" alt=""><br>用户2点击过url2,url3,这是特征,label是他点击了该广告,于是是1.<br>另外一个是看网上的CTR应用,特征数据其实是物品本身的特征加上其现有的点击率作为label来训练,到来了一个新的物品,根据其特征来<strong>预测这个物品的综合点击率</strong>,不过这种做法对推荐系统个性化需求是没啥意义的…<br>在推荐系统中,我们也可以根据稀疏的行为矩阵来做,每个物品建立预测模型..预测用户会不会点击物品.但是行为得选适合的行为来表示用户这个行为的确代表对物品感兴趣.</p>
<blockquote>
<p>随机梯度下降法每次只关注一个数据点,每次迭代至下一个数据点都会根据所得到的信息更新参数估计值,知道穷尽所有数据点.适合于应用于大数据和稀疏特征矩阵的场景.缺点是有时候优化效果一般且依赖于步长λ的设定.  </p>
</blockquote>
<p>其有两个大优点</p>
<ul>
<li>作为线性模型,训练时间比KNN这样的快</li>
<li>可解释性<br>  其与线性回归最大的不同的是预测值在(0,1)之间(就像朴素贝叶斯),可以作为概率来使用,而线性回归给的是(-∞,+∞)之间的一个值.<br>  为什么会这样?因为数据的特征矩阵被一个函数巧妙地转换到了严格位于[0,1]之间的数值.这样一个函数当然定义域为R,值域为[0,1].这个函数就是<strong>sigmoid</strong>了.其导数还就是<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.34.44.png" alt=""></li>
</ul>
<p><strong>正式地说明</strong>,从线性回归开始,假设我们认为数据集对应的输出标记是在指数尺度上变化,于是也可以变换y,得到:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.45.00.png" alt="">    ….(1)<br>这个叫<strong>对数线性回归</strong>,推广到一般情况,用g()对y做变换,即g^-1对自变量做变换,得到广义线性模型<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.48.33.png" alt=""></p>
<p>考虑二分类任务,样本们的label要么是0要么是1,所以我们想要变换能把线性回归的结果z转换为0,1值,于是需要一个g()变换,而我们可以采用采用阶跃函数,但是阶跃函数又不好,于是就用sigmoid,而这样的话预测值就落在(0,1)之内,这正好还可以看作概率….<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2022.32.04.png" alt=""><br>其实不管是用哪个函数 在变换前线性回归的层面上都是正例回归出的值(z)大于0,负例回归出的值小于0 通过训练就可以达到这种效果<br>我们看做<strong>y是代表正例概率,然后正例概率大就认为是正例咯</strong>…于是就可以使用sigmoid函数转化.将其作为g^-1代入,可以得到:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.50.58.png" alt="">,y可以按任务特性表示为<img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2016.02.04.png" alt=""><br>也就是:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.51.19.png" alt=""><br>由于y代表的是概率,不妨把y视为x为正例的概率,那么1-y就是反例可能性.于是我们拟合的是两者比值的对数.称为<strong>对数几率</strong>,也就是z.<br>z=<img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2015.52.57.png" alt=""><br>所以,这个式子在用线性模型的结果z取逼近真实标记的<strong>对数几率</strong>.</p>
<p>另一个特别的地方在于求解,这个式子无解析解,得使用数值计算方法.思路是通过<strong>极大似然函数建立模型</strong>,然后转换到负对数似然函数(凸函数),然后通过SGD方法来最小化极大似然函数.进而求解w,b….具体看书吧😎….</p>
<p>(一个关键的问题是有多少物品你就得建立多少个模型取预测….)</p>
<p><strong>其建立的线性关系是样本和正例概率的对数几率间的线性关系…而自变量和概率y的关系是一种非线性关系了…看(1)式的那段话就知道如何计算到底是怎样的关系了</strong></p>
<h3 id="垃圾邮件过滤与朴素贝叶斯model"><a href="#垃圾邮件过滤与朴素贝叶斯model" class="headerlink" title="垃圾邮件过滤与朴素贝叶斯model"></a>垃圾邮件过滤与朴素贝叶斯model</h3><p>Why not linear regreesion?</p>
<ul>
<li>首先,这是个分类任务,线性回归不好做,阈值也不好取</li>
<li>难以打标签</li>
<li>特征个数比样本还多,这让线性回归模型实在是不好做.</li>
</ul>
<p>Why not KNN?</p>
<ul>
<li>最主要的问题是维度太多,最近的邻居也隔得很远.</li>
<li>计算量问题</li>
</ul>
<p>首先是基于个别单词的过滤:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2016.36.44.png" alt=""><br>右边各个都很好通过有标签数据统计出来,整个公式也不要再多讲了.</p>
<p>然后是基于特征向量:<br>首先要<strong>假设特征向量的各个维度是独立</strong>的,于是可以把贝叶斯公式中的p(多个word|spam)分解开来,得到:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2016.40.16.png" alt=""><br>对于P(xi|c)你看他是离散的还是连续的,使用概率质量或者概率密度即可<br>分类的时候哪个分类概率最大你就拿哪个就好了.</p>
<p>拉普拉斯平滑:拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题.思想很简单,直接上图了:<br><img src="/2017/12/11/数据科学实战/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-11%2016.50.56.png" alt=""></p>
<h3 id="逻辑回归新解"><a href="#逻辑回归新解" class="headerlink" title="逻辑回归新解"></a>逻辑回归新解</h3><p>先从线性回归看,我们使用线性回归模型去预测了一个z,z的值在(-∞,+∞).而我们需要做<strong>二分类</strong>,所以需要把预测值z映射到y=(0,1)之间,而y正好可以代表为正例的概率.</p>
<p>sigmoid函数正是在z和y之间做这么个转换…由于:<br><img src="/2017/12/11/数据科学实战/38BB27D3-ED71-4AC3-8E13-C98C6BC440CE.png" alt=""><br>于是<br>z=<img src="/2017/12/11/数据科学实战/BA8F2A56-E376-4876-929B-448A30095614.png" alt="">,所以数学上我们线性模型预测的z也就是<strong>对数几率”</strong><br>所以我们的线性回归模型是在预测<img src="/2017/12/11/数据科学实战/BA8F2A56-E376-4876-929B-448A30095614.png" alt="">,我们只要让最后程序的输出是y不是z就好…<br>所以在逻辑回归时,我们要拟合出下式<br><img src="/2017/12/11/数据科学实战/FE73306C-3C27-479E-A67D-1F55E0EA9913.png" alt=""><br>解出的y就是概率了…使用最大似然法来解…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/06/冷启动问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/06/冷启动问题/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-06T15:39:14+08:00">
                2017-12-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="冷启动问题"><a href="#冷启动问题" class="headerlink" title="冷启动问题"></a>冷启动问题</h1><p>#推荐引擎/推荐系统实践</p>
<ul>
<li>用户冷启动</li>
<li>物品冷启动:如何把没有被行为的新物品推出去</li>
<li>系统冷启动</li>
</ul>
<p>解决思路</p>
<ol>
<li>先推热门物品,然后再个性化推荐</li>
<li>利用注册信息,比如年龄什么的做粗粒度的个性化推荐</li>
<li>用户注册的时候给出兴趣反馈<br>但对于引擎,你只好用(1)</li>
<li>物品冷启动,可以基于<strong>内容</strong>,不需要有什么行为就能把新物品推出去.</li>
<li>系统冷启动可能就得人工打造物品相关度表.</li>
</ol>
<h3 id="利用用户META-☆明天实现"><a href="#利用用户META-☆明天实现" class="headerlink" title="利用用户META(☆明天实现)"></a>利用用户META(☆明天实现)</h3><p>思想也很简单</p>
<ol>
<li>按用户meta给用户分类(最好是离散特征或者把连续特征离散化)(最简单地情况只使用一个分类把用户分为两类,这很粗糙但是相比不分,精度已经大大提高了.).比如指定使用职业,性别分类,算法里将读取user_meta中的信息,将用户分类,进而将行为分类,然后第2步.</li>
<li>算法统计分类用户中的热门物品,得到defaultList: 此时的defaultList 就代表多物品推荐了,其中key为分类名,value为该分类的推荐列表</li>
<li>给用户推荐其分类中的的热门物品,产生user_item_rec_list,由于用户有多分类,所以所以需要其多分类的推荐结果<strong>加权.</strong>,进行topn</li>
</ol>
<p>统计分类用户中的热门物品又将对不热门物品不友好,而推荐热门物品并不是推荐系统的主要任务.可以定义p(f,i)(具有特征f的用户对i的喜好)为:</p>
<blockquote>
<p><img src="/2017/12/06/冷启动问题/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-06%2016.54.12.png" alt="">也就是<strong>喜欢i的用户中具有特征f的比例(即具有f特征用户比例最高的物品)</strong>.α可以解决数据稀疏,比如i只被1个用户喜欢过,这个用户又正好f了,那么p就为1.选择大α可以解决这问题.Ni是喜欢i的用户集合,Uf是具有特征f的用户集合.  </p>
</blockquote>
<p>在实践中,我们改变一下概念,改为计算”i的总行为次数”(分母),含特征f对i的行为次数(分子).u</p>
<h3 id="利用用户预提供兴趣"><a href="#利用用户预提供兴趣" class="headerlink" title="利用用户预提供兴趣"></a>利用用户预提供兴趣</h3><p>见书吧..这个不在引擎的考虑范围就不做笔记了</p>
<h3 id="利用content做物品冷启动"><a href="#利用content做物品冷启动" class="headerlink" title="利用content做物品冷启动."></a>利用content做物品冷启动.</h3><p>对于userCF,物品冷启动并不是太敏感的问题,因为总有人看到这个物品,那么和他相似的人进而也会看到…但是对ItemCF就很不好了.</p>
<blockquote>
<p>KNN(k near neighborhood)  </p>
<ol>
<li>利用tfidf是第一种思路,这个已经实现了.</li>
<li>LDA:我们需要考虑,长文本的时候tfidf很好,但是短文本的时候,有时候两句话表示一个意思,却没有相同的关键词,这时候就需要topic model了<br>LDA可以看推荐书,下面给出一个简要的说明是干啥:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">LDA要干的事情简单来说就是为一堆文档进行聚类（所以是非监督学习），一种topic就是一类，要聚成的topic数目是事先指定的。聚类的结果是一个概率，而不是布尔型的100%属于某个类。国外有个博客[1]上有一个清晰的例子，直接引用：</span><br><span class="line"></span><br><span class="line">Suppose you have the following set of sentences:</span><br><span class="line"></span><br><span class="line">1. I like to eat broccoli and bananas.</span><br><span class="line">2. I ate a banana and spinach smoothie for breakfast.</span><br><span class="line">3. Chinchillas and kittens are cute.</span><br><span class="line">4. My sister adopted a kitten yesterday.</span><br><span class="line">5. Look at this cute hamster munching on a piece of broccoli.</span><br><span class="line"></span><br><span class="line">What is latent Dirichlet allocation? It’s a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like</span><br><span class="line"></span><br><span class="line">* Sentences 1 and 2: 100% Topic A</span><br><span class="line">* Sentences 3 and 4: 100% Topic B</span><br><span class="line">* Sentence 5: 60% Topic A, 40% Topic B</span><br><span class="line">* Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)</span><br><span class="line">* Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)</span><br><span class="line"> 它当然没法儿自动描述topic,但是它能给出词频,你可以人工根据这个推测这个topic是个啥.</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<p>例子里的结果，除了为每句话得出了一个概率的聚类结果，而且对每个Topic，都有代表性的词以及一个比例。以Topic A为例，就是说<strong>所有对应到Topic A的词里面，有30%的词是broccoli。</strong>在LDA算法中，会把<strong>每一个文档中的每一个词对应到一个Topic</strong>，所以能算出上面这个比例。这些词为描述这个Topic起了一个很好的指导意义，我想这就是LDA区别于传统文本聚类的优势吧。</p>
<h3 id="LDA调研"><a href="#LDA调研" class="headerlink" title="LDA调研"></a>LDA调研</h3><p>下面是一个sparkLDA的例子<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhengpeiwei on 2017/12/6.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LDADFTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkSession spark=SparkSession.builder().master(<span class="string">"local"</span>).appName(<span class="string">"tf"</span>).getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Loads data.</span></span><br><span class="line">        Dataset&lt;Row&gt; dataset = spark.read().format(<span class="string">"libsvm"</span>)</span><br><span class="line">                .load(<span class="string">"/Users/zhengpeiwei/Desktop/ldasvm.txt"</span>);</span><br><span class="line">        dataset.show();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Trains a LDA model.</span></span><br><span class="line">        LDA lda = <span class="keyword">new</span> LDA().setK(<span class="number">3</span>).setMaxIter(<span class="number">10</span>);</span><br><span class="line">        LDAModel model = lda.fit(dataset);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> ll = model.logLikelihood(dataset);</span><br><span class="line">        <span class="keyword">double</span> lp = model.logPerplexity(dataset);</span><br><span class="line">        System.out.println(<span class="string">"The lower bound on the log likelihood of the entire corpus: "</span> + ll);</span><br><span class="line">        System.out.println(<span class="string">"The upper bound on perplexity: "</span> + lp);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Describe topics.</span></span><br><span class="line">        Dataset&lt;Row&gt; topics = model.describeTopics(<span class="number">3</span>);</span><br><span class="line">        System.out.println(<span class="string">"The topics described by their top-weighted terms:"</span>);</span><br><span class="line">        topics.show(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Shows the result.</span></span><br><span class="line">        Dataset&lt;Row&gt; transformed = model.transform(dataset);</span><br><span class="line">        transformed.show(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中transformed的数据如下:<br><img src="/2017/12/06/冷启动问题/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-06%2019.17.39_800x197.png" alt=""><br>可以看出,topicDistribution给出了每个文档是哪个topic的概率,feature是一个稀疏向量,11代表其维度,每个位置上的词可以看做<strong>词袋模型</strong>给出的词频,这个词频是可以用tf生成的.<br>东西还是很好理解,但是网上的分析说<strong>其效果并不算好,普遍地还不如tfidf,特别是对短文本,但是你搭配tdidf做还是有微小提高的</strong>.</p>
<p>度量:<br><img src="/2017/12/06/冷启动问题/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-06%2019.27.56.png" alt=""></p>
<p>把物品做了LDA之后,每篇文章都有一个话题上的概率分布,则物品间两两计算相似度可以使用KL散度:</p>
<blockquote>
<p><img src="/2017/12/06/冷启动问题/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-06%2019.22.54.png" alt="">,其中i是第i个topic<br>KL散度(相对熵):我们现在用的是两个离散概率分布,<br>相对熵是用来度量使用基于q的编码来编码来自p的样本平均所需的额外的比特个数,在一定程度上，熵可以度量两个随机变量的距离。  </p>
<ul>
<li>KL散度不具有对称性</li>
<li>相对熵的值是非负的,当两个随机分布相同时，它们的相对熵为零.</li>
</ul>
</blockquote>
<p>基本熵:如果一个随机变量的可能取值为，对应的概率为，则随机变量的熵定义为:<br><img src="/2017/12/06/冷启动问题/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-06%2019.32.33.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/29/CTR与逻辑回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/29/CTR与逻辑回归/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-29T22:05:15+08:00">
                2017-11-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CTR与逻辑回归"><a href="#CTR与逻辑回归" class="headerlink" title="CTR与逻辑回归"></a>CTR与逻辑回归</h1><p>#推荐引擎/论文</p>
<h2 id="一个介绍"><a href="#一个介绍" class="headerlink" title="一个介绍"></a>一个介绍</h2><p><a href="http://blog.csdn.net/xiewenbo/article/details/37994163" target="_blank" rel="noopener">网页</a></p>
<blockquote>
<p>CTR=被点击次数/展示次数  ===&gt;点击率  </p>
</blockquote>
<p><strong>数据是一系列广告的特征与其点击次数,未点击次数(即现有点击率,即为”1”的概率),每一行代表一个广告.CTR问题训练模型,然后对新的广告(特征向量),预测其点击率即可.高的就排前头,与请求无关.而LR不仅可以给出分类值,还可以给出概率,于是就用到概率作为需要的CTR预估值.</strong> </p>
<p>逻辑回归可以用在CTR(Click Through Rate)预估上，即通常所说的点击率预估。点击率预估的意义在于，搜索引擎等广告平台想要赚更多的钱，就要通过某一种机制让赚钱最多的广告排在前面（或有更多的概率被展示）。<br>一、排序规则<br>为了获得更多的收益，一般搜索引擎、广告联盟的排序规则是：<br>其中$bidPrice$是指广告主给出的竞拍价格，$CTR$就是我们预估的该广告的点击率，总体结果越高越容易被展示。<br>当然，这个最终的分数计算还有其他的规则，这里只是列出具CTR预估在这里的重要作用。<br>二、逻辑回归<br>我们依然使用之前在逻辑回归中用到的$sigmoid$函数作为模型：<br>含义为，我们<strong>给出一个查询Q和一个广告，预测其被点击(y=1)的概率,到时候来了一个query,就可以根据点击率排广告顺序</strong>。<br>我们的特征数据包括：广告质量得分、广告创意得分、Query与广告的相关性、相对价格、相对成交量等等，具体这些特征的值如何获得又是另外的课题，这里暂不涉及。<br>有了特征数据，现在我们有一批数据如下图所示：<br>0 20 0.294181968932 0.508158622733 0.182334278695 0.629420618229<br>0 68 0.1867187241 0.606174671096 0.0748709302071 0.806387550943<br>0 18 0.62087371082 0.497772456954 0.0321750684638 0.629224616618<br>1 90 0.521405561387 0.476048142961 0.134707792901 0.400062294097<br>0 75 0.0126899618353 0.507688693623 0.377923880332 0.998697036848<br>0 8 0.308646073229 0.930652495254 0.755735916926 0.0519441699996<br>0 64 0.444668888126 0.768001428418 0.501163712702 0.418327345087<br>0 79 0.842532595853 0.817052919537 0.0709486928253 0.552712019723<br>1 32 0.410650495262 0.164977576847 0.491438436479 0.886456782492<br>其中第一列是正样本（被点击）的个数，第二列是负样本（展示但未点击个数）。<br>三、逻辑回归<br>关于逻辑回归的原理可以参考我之前的文章，我们会发现这里的数据与之前的不同，每一行不再是一个单独的记录，而是一组记录的统计，这种形式在实践中更容易计算，并且更节省存储空间。<br>四、R逻辑回归<br>我们首先把数据读取到内存中，存储于ctr_data变量中：<br>ctr_data = read.csv(‘CTR_DATA.txt’,header=F,sep=” “)<br>看一下里面的数据：</p>
<blockquote>
<p>head(ctr_data)<br>  V1 V2         V3        V4         V5         V6<br>1  0 20 0.29418197 0.5081586 0.18233428 0.62942062<br>2  0 68 0.18671872 0.6061747 0.07487093 0.80638755<br>3  0 18 0.62087371 0.4977725 0.03217507 0.62922462<br>4  1 90 0.52140556 0.4760481 0.13470779 0.40006229<br>5  0 75 0.01268996 0.5076887 0.37792388 0.99869704<br>6  0  8 0.30864607 0.9306525 0.75573592 0.05194417<br>把该变量添加到环境变量中，这样后面使用其中的字段就可以直接写了：<br>attach(ctr_data)<br>最重要的一步，根据数据生成逻辑回归模型：<br>ctr_logr = glm(cbind(V1,V2)~V3+V4+V5+V6,family=binomial(link=”logit”))<br>其中$y$~$x {1}+x {2}$的意思是根据$x {1}$、$x {2}$来预测y出现的概率。<br>我们新创建一个数据集，对其出现的概率(即V1所代表的含义)进行预测：<br>record = data.frame(V3=0.294181968932,V4=0.508158622733,V5=0.182334278695,V6=0.629420618229)<br>d &lt;- predict(ctr_logr, newdata = record, type = “response”)<br>1<br>0.004845833<br>可以清楚地看到，该特征向量(即一个广告)被点击的概率是0.00484，也就是说大约展示250次可能会被点击一次。</p>
</blockquote>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-29T11:03:25+08:00">
                2017-11-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Collaborative-Filtering-for-Implicit-Feedback-Datasets"><a href="#Collaborative-Filtering-for-Implicit-Feedback-Datasets" class="headerlink" title="Collaborative Filtering for Implicit Feedback Datasets"></a>Collaborative Filtering for Implicit Feedback Datasets</h1><p>#推荐引擎/论文</p>
<p>采用的是有数值的隐数据 数值以行为不同而不同 然后有数值者标记为1 否则为0 在综合可信度 优化的时候综合标记和可信度优化</p>
<p>在这文章中,我们提出把数据当做indication of positive and negative preference,并给出不同的<strong>置信度</strong>,进而给出一个因子模型.并且还建议了一种scalable optimizaion procedure,其和数据大小scales linearly. 最后,还给出了一种新颖的方式来解释因子模型给出的推荐.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>一个隐式反馈:一个买了很多某作者书的人可能就喜欢这个作者,在此,buy就是隐式反馈.作为本文的例子,研究了一个TV show推荐,没有显式反馈,只能分析<strong>watching habit</strong>s of 匿名用户.隐式反馈的主要特征:</p>
<ol>
<li>无negative feedback.</li>
<li>隐式反馈is inherently <strong>noisy</strong>.比如在电商中,”view”行为就不能说明啥.看一个节目看的久也可能是用户睡着了..</li>
<li>numerical在显式反馈中表示喜好,而在隐式反馈中表示<strong>置信度</strong>.其表示了action的frequency,比如看show的时间,用户买一个东西的频率.大value并不就表示higher preference.比如用户最爱的电影他可能就只看一次,但是一个普通的series show他每周都要看.但是也是能说明置信度的:A one time event might be caused by various reasons that have nothing to do with user preferences. However, a recurring event is <strong>more likely(而非一定)</strong> to reﬂect the user opinion.</li>
<li>对隐式反馈推荐系统的评估需要合适方法.</li>
</ol>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><blockquote>
<p>The input data associate users and items through <strong>rui</strong> values,称作<strong>observation</strong>.在显式数据集中可能是评分;隐式数据集中表示<strong>observations for user actions,</strong>比如看TV show的次数或者看网页的时间.0.7表示u看了70%的show,2表示完整看了两次.不同的数据里rui的含义不同  </p>
</blockquote>
<p>在隐式数据集中,”评分”矩阵中::每个值都是非空的::,没有action就是0.</p>
<h2 id="Our-Model"><a href="#Our-Model" class="headerlink" title="Our Model."></a>Our Model.</h2><p>首先使用pui表示推测用户u喜欢不喜欢物品i.<br><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2014.53.32.png" alt=""><br>然后由于行为不一定代表喜欢,就给一个置信度,次数越多越可能说明喜欢<br><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2014.54.11.png" alt=""><br>α取40是一个经验.</p>
<p>模型的目标是找到用户向量xu和物品向量yi来表示factors.然后用户对物品的preference就是内积.这很像一般隐因子模型,但是又两点大不同</p>
<ul>
<li>需要考虑置信度</li>
<li>Optimization should account for all possible u, i pairs, rather than only those corresponding to observed data.<br>于是,我们最小化下面的损失函数以得到xu,yi<blockquote>
<p><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2014.58.54.png" alt="">,注意置信度c  </p>
</blockquote>
</li>
</ul>
<p>由于矩阵无缺失值,这个函数将包含U*I项,这项也太多了,搞得随机梯度下降计算的太多了,所以推出下面的优化方法:</p>
<p>当要么xu,要么yi是固定的时候,损失函数就是一个二次方程组了,最小值就能比较轻松地计算.这就引出了::ALS优化方法:::交替地计算xu和yi,每一步都能保证lower损失函数值.显式反馈也用这个,把unknown values就不管,处理的是稀疏矩阵.对隐式反馈矩阵就是稠密的了,并且有置信度.下面详细解释</p>
<h3 id="ALS的优化过程"><a href="#ALS的优化过程" class="headerlink" title="ALS的优化过程"></a>ALS的优化过程</h3><blockquote>
<p>n—&gt;物品数量  </p>
</blockquote>
<p>第一步,重新计算所有用户向量xu. 让我们把所有物品向量yi聚合成Y矩阵.首先计算YTY,时间是O(f^2<em>n).对**每个用户又定义n</em>n的对角矩阵Cu,其中Cii=cui<strong>;我们找到了一个xu的计算公式能最小化损失函数:<br><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2015.38.30.png" alt=""><br>I是单位矩阵.这个公式的瓶颈在于计算YTCuY,因为其和每个用户都有关,这计算量太大了.但是有一个事实:<br><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2015.39.36.png" alt=""><br>现在,YTY可以提前计算好,对所有用户都一样.而(Cu-I)是对角阵减一个单位阵(即减掉置信度中的1),其中顶多有nu个元素不为0(nu为是rui&gt;0(有过行为)的物品个数).类似的,Cu<em>p(u)也顶多是nu个不为0元素.于是,对每个用户,xu的重计算时间是O(f^2 </em>nu + f^3),其中f^3是求逆的时间.对所有用户就是<img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2015.46.34.png" alt="">where N is the overall number  of non-zero observations.可以看出,其对输入规模是</strong>线性的**,而Typical values of f lie between 20 and 200,也不是很大.</p>
<p>第二步重计算物品向量yi.和重计算用户向量是<strong>对称</strong>的.在此贴一下论文:<br><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2015.50.14.png" alt=""></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ol>
<li>可以设置pui的门槛,需要rui到达某个程度才pui=1.</li>
<li>可以改变cui的计算公式:<blockquote>
<p><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2015.30.58.png" alt="">  </p>
</blockquote>
</li>
</ol>
<h2 id="读完这篇后对SVDPP中隐式反馈部分的重新理解"><a href="#读完这篇后对SVDPP中隐式反馈部分的重新理解" class="headerlink" title="读完这篇后对SVDPP中隐式反馈部分的重新理解"></a>读完这篇后对SVDPP中隐式反馈部分的重新理解</h2><p><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2016.03.03.png" alt=""><br>观察SVD++的公式,yj是物品的另一个特征向量,而在公式中这个值是考虑用户提供过隐式反馈的物品j的yj之和.</p>
<p>我们把”有过行为”看做一种隐式反馈.那么SVD++的公式意思是,由q,p预测用户对i的喜爱,但是还不够,还需要估算i和用户喜欢的那些j之间的preference度也加进来.**,其中|N(u)|^-0.5为了防止该项过大.</p>
<blockquote>
<p><img src="/2017/11/29/Collaborative Filtering for Implicit Feedback Datasets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%2016.06.02.png" alt=""><br>当然,这种隐式反馈取得比较简单….</p>
</blockquote>
<p>在Spark的实现中,提供过隐式反馈的物品|N(u)|取得就是<strong>打过分的物品总个数</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">PW</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PW</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
