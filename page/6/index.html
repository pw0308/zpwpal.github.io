<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="PW&#39;s notes">
<meta property="og:url" content="http://yoursite.com/page/6/index.html">
<meta property="og:site_name" content="PW&#39;s notes">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PW&#39;s notes">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/6/"/>





  <title>PW's notes</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PW's notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/13/集合/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/13/集合/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-13T12:50:39+08:00">
                2017-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h1><p>#scala</p>
<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用List()函数创建List,实际是List伴生对象的apply()方法</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> numbers=<span class="type">List</span>(<span class="number">12</span>,<span class="number">123</span>,<span class="number">23123</span>,<span class="number">111</span>)</span><br><span class="line">numbers: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">12</span>, <span class="number">123</span>, <span class="number">23123</span>, <span class="number">111</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//可以使用下标访问</span></span><br><span class="line"><span class="keyword">val</span> s=numbers(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//一种遍历方法</span></span><br><span class="line"><span class="keyword">var</span> i=numbers</span><br><span class="line"><span class="keyword">while</span>(!i.empty)&#123;print(...);i=i.tail&#125;</span><br><span class="line"><span class="comment">//或者</span></span><br><span class="line"><span class="keyword">while</span>(!i=<span class="type">Nil</span>)&#123;print(...);i=i.tail&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>size</li>
<li>head</li>
<li>tail(返回新的集合,包括除了第一个元素的所有元素)</li>
<li>还有更多List的低阶算数运算函数,参考<a href="http://www.scala-lang.org/api/2.10.4/" target="_blank" rel="noopener">scaladoc</a></li>
<li>映射列表函数</li>
<li>规约列表函数</li>
<li>转换集合函数{toList(),toSet(),toMap()..}</li>
<li><p><strong>想象一下rdd是集合,List也是集合,所以也有map,filter,reduce等高阶1函数可用</strong><br>Range()其实也是一个数字List,于是,对于for我们可以使用List作为循环变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i&lt;- numbers)</span><br></pre></td></tr></table></figure>
<p>高阶函数的使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//传入lambda即可.</span></span><br><span class="line"> collection.map(item:<span class="class"><span class="keyword">type</span> <span class="title">=&gt;</span> <span class="title">op</span> <span class="title">on</span> <span class="title">items</span>)</span></span><br><span class="line"><span class="class"><span class="title">//foreach</span>(<span class="params"></span>)<span class="title">=&gt;右边是是一个函数</span>(<span class="params">过程</span>),<span class="title">对每个元素调用这个函数</span></span></span><br><span class="line"><span class="class"><span class="title">//map</span> <span class="title">将一个列表元素转换为另一个值/类型</span></span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h3><p>自然是无序,不重复.</p>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>map里的键值对自然是二元组,于是使用元组语法-&gt;来定义kv对.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> cmap=<span class="type">Map</span>(<span class="string">"red"</span>-&gt;<span class="number">16000</span>,<span class="string">"green"</span>-&gt;<span class="number">50</span>)</span><br><span class="line">cmap: scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(red -&gt; <span class="number">16000</span>, green -&gt; <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> value=cmap(<span class="string">"red"</span>)</span><br><span class="line">value: <span class="type">Int</span> = <span class="number">16000</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> has=cmap.contains(<span class="string">"weq"</span>)</span><br><span class="line">has: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">for</span>(item&lt;-cmap)&#123;println(item)&#125;</span><br><span class="line">(red,<span class="number">16000</span>)</span><br><span class="line">(green,<span class="number">50</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Java和Scala兼容"><a href="#Java和Scala兼容" class="headerlink" title="Java和Scala兼容"></a>Java和Scala兼容</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="comment">//有这个命令就导入了手动转换J/S集合的功能.</span></span><br></pre></td></tr></table></figure>
<h3 id="集合的模式匹配"><a href="#集合的模式匹配" class="headerlink" title="集合的模式匹配"></a>集合的模式匹配</h3><p>集合可以用来做匹配来case集合是什么样子.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">list <span class="keyword">match</span>&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">List</span>(<span class="number">111</span>,<span class="number">222</span>)=&gt;...</span><br><span class="line"><span class="keyword">case</span> <span class="type">List</span>(<span class="number">111</span>,<span class="number">223</span>)=&gt;...</span><br><span class="line"><span class="keyword">case</span> <span class="type">List</span>(<span class="number">123</span>,<span class="number">123</span>)=&gt;...</span><br><span class="line"><span class="keyword">case</span> <span class="type">List</span>(x,<span class="number">121</span>)=&gt;<span class="comment">//这里的x是值绑定,这种又不是数字又不是串的明显的变量名符号就是值绑定了,代表某个元素,只要符合(某个元素,121)的就能匹配上,而=&gt;右边可以使用x取得其值操作....</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="More-Collections"><a href="#More-Collections" class="headerlink" title="More Collections"></a>More Collections</h2><h3 id="mutable包"><a href="#mutable包" class="headerlink" title="mutable包"></a>mutable包</h3><p>集合默认是不可变的.scala的集合系统的区分了可变（ mutable  ）和不可变（immutable ）集合。一个mutable  集合能够更新甚至扩展空间，这意味着你能改变，增加，或者删除一个集合的元素。 一个immutable集合，刚好相反，不能改变。<strong>你仍然可以做一些类似的增加，删除，或者更新，但是实际上他返回了一个新的对象</strong>.这样可以保证比如别的线程里对原来那个集合的引用引用到的东西还是不变的..</p>
<h3 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h3><p>用来储存固定大小的同类型元素.可以先声明指定长度,然后赋值;使用aaply()也是可以的.同样可以应用<strong>高阶函数</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> z:<span class="type">Array</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">3</span>)/</span><br><span class="line"><span class="keyword">var</span> z = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">3</span>)</span><br><span class="line">z(<span class="number">0</span>) = <span class="string">"Runoob"</span>; z(<span class="number">1</span>) = <span class="string">"Baidu"</span>; z(<span class="number">4</span>/<span class="number">2</span>) = <span class="string">"Google"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> z = <span class="type">Array</span>(<span class="string">"Runoob"</span>, <span class="string">"Baidu"</span>, <span class="string">"Google"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//也可以创建区间数组</span></span><br><span class="line"><span class="keyword">var</span> myList1 = range(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>),<span class="number">2</span>是步长</span><br><span class="line"><span class="keyword">var</span> myList2 = range(<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出所有数组元素</span></span><br><span class="line">      <span class="keyword">for</span> ( x &lt;- myList ) &#123;</span><br><span class="line">         println( x )</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算数组所有元素的总和,或者直接使用sum().</span></span><br><span class="line">  <span class="keyword">var</span> total = <span class="number">0.0</span>;</span><br><span class="line">      <span class="keyword">for</span> ( i &lt;- <span class="number">0</span> to (myList.length - <span class="number">1</span>)) &#123;</span><br><span class="line">         total += myList(i);</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"总和为 "</span> + total);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 查找数组中的最大元素,或者使用max()</span></span><br><span class="line">      <span class="keyword">var</span> max = myList(<span class="number">0</span>);</span><br><span class="line">      <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> to (myList.length - <span class="number">1</span>) ) &#123;</span><br><span class="line">         <span class="keyword">if</span> (myList(i) &gt; max) max = myList(i);</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"最大值为 "</span> + max);</span><br></pre></td></tr></table></figure></p>
<p>我们也可以使用多维数组,声明如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> myMatrix = ofDim[<span class="type">Int</span>](<span class="number">3</span>,<span class="number">3</span>)<span class="comment">//ofDim方法用以创建指定长度数组</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">// 创建矩阵</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to <span class="number">2</span>) &#123;</span><br><span class="line">         <span class="keyword">for</span> ( j &lt;- <span class="number">0</span> to <span class="number">2</span>) &#123;</span><br><span class="line">            myMatrix(i)(j) = j;</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Option-Try单元素集合"><a href="#Option-Try单元素集合" class="headerlink" title="Option/Try单元素集合"></a>Option/Try单元素集合</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/11/函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/11/函数/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-11T17:21:46+08:00">
                2017-07-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><p>#scala</p>
<p>用函数还是要加()的….</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义函数,要指定类型</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">multi</span></span>(x:<span class="type">Int</span>,y:<span class="type">Int</span>):<span class="type">Int</span>=&#123;x*y&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义过程:没有返回值的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(d:<span class="type">Double</span>*) = println(<span class="symbol">'hell</span>o')<span class="comment">//注意*代表可变参数,于是函数可以有多个Double参数</span></span><br><span class="line">log &#123;<span class="keyword">val</span> a=<span class="number">1</span>; a+<span class="number">1</span>+<span class="number">2</span>&#125;<span class="comment">//可以使用&#123;&#125;表达式块包含参数来调用函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//递归函数前加上下面,系统会对递归函数做优化.(递归调用得是最后一条语句的时候)</span></span><br><span class="line"><span class="meta">@annotation</span>.tailrec</span><br><span class="line"></span><br><span class="line"><span class="comment">//函数可以嵌套定义,比如下面,同时注意参数是可以写为()()参数组的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>,c:<span class="type">Int</span>)=&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>)= <span class="keyword">if</span>(x&gt;y) x <span class="keyword">else</span> y <span class="comment">//if else也是语句块 本身支持返回最后一条语句的值</span></span><br><span class="line">	max(a,max(b,c))</span><br><span class="line"></span><br><span class="line"><span class="comment">//类型参数,在函数名后加[type-name],因为你不能用Any做任意类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity</span></span>(a:<span class="type">Any</span>):<span class="type">Any</span>=a<span class="comment">//✘这样你使用val s:String=identity('hello')是通不过的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity</span></span>[<span class="type">A</span>](a:<span class="type">A</span>):<span class="type">A</span>=a<span class="comment">//√</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//方法是class中的函数,使用.调用 或者使用  object method-name (params)</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><blockquote>
<p>函数的类型是 (params) =&gt; return type  </p>
</blockquote>
<p>函数可以储存在值中<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义一个函数并赋给一个变量</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">double</span></span>(x:<span class="type">Int</span>):<span class="type">Int</span> = x*<span class="number">2</span></span><br><span class="line">double: (x: <span class="type">Int</span>)<span class="type">Int</span> <span class="comment">//使用def时编译器这么提示类型,使用lambda的时候会是Int=&gt;Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> x:<span class="type">Int</span>=&gt;<span class="type">Int</span>=double</span><br><span class="line">x: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1193</span>/<span class="number">1889468930</span>@<span class="number">654e6</span>a90<span class="comment">//注意x的类型</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> y=double _ <span class="comment">//或者可以不需要声明类型 但是需要 _标识函数调用</span></span><br><span class="line">y: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1197</span>/<span class="number">1550471570</span>@<span class="number">1</span>c8f6c66</span><br><span class="line"></span><br><span class="line">scala&gt; x(<span class="number">5</span>)</span><br><span class="line">res2: <span class="type">Int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>高阶函数包含一个<strong>函数类型</strong>的值作为参数/返回值,于是可以把函数传递给它.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">op</span></span>(s:<span class="type">String</span>,f:<span class="type">String</span>=&gt;<span class="type">String</span>)=&#123;</span><br><span class="line">     | <span class="keyword">if</span>(s==<span class="literal">null</span>) s <span class="keyword">else</span> f(s) </span><br><span class="line">     | &#125;</span><br><span class="line">op: (s: <span class="type">String</span>, f: <span class="type">String</span> =&gt; <span class="type">String</span>)<span class="type">String</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(s:<span class="type">String</span>)=s.reverse</span><br><span class="line">f: (s: <span class="type">String</span>)<span class="type">String</span></span><br><span class="line"></span><br><span class="line">scala&gt; op(<span class="string">"hello"</span>,f)<span class="comment">//可以使用定义好的f,**也可以使用string=&gt;string的lambda表达式来作为参数**</span></span><br><span class="line">res3: <span class="type">String</span> = olleh</span><br></pre></td></tr></table></figure></p>
<h3 id="Lambda表达式-看做无名函数"><a href="#Lambda表达式-看做无名函数" class="headerlink" title="Lambda表达式(看做无名函数):"></a>Lambda表达式(看做无名函数):</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//不像函数写def,直接写val即可,因为你把它看做"函数字面值"</span></span><br><span class="line"><span class="keyword">val</span> name = (&lt;identifier&gt;:&lt;<span class="class"><span class="keyword">type</span><span class="title">&gt;</span>,<span class="title">…</span>) <span class="title">=&gt;</span> <span class="title">&lt;expressions&gt;</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">(<span class="params">a:<span class="type">Int</span>,b:<span class="type">Int</span></span>)<span class="title">=&gt;a+b//</span>,<span class="title">甚至无参数</span></span></span><br><span class="line"><span class="class">(<span class="params"></span>)<span class="title">=&gt;”hello</span>"<span class="title">//在函数类型已经显示指定好的情况下</span>,<span class="title">甚至可以</span></span></span><br><span class="line"><span class="class"><span class="title">s=&gt;s</span>.<span class="title">reverse//</span>,<span class="title">这里s是String</span>.</span></span><br></pre></td></tr></table></figure>
<p>占位符语法_ 是lambda的缩写形式,表示函数命名参数,需要有一下条件</p>
<ul>
<li>函数显式类型在lambda之外指定了</li>
<li>参数在expression内最多使用一次<br>实际上就是 <strong>()=&gt;()把左边和=&gt;省略了只写右边</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//定义函数时使用一个_</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> d:<span class="type">Int</span>=&gt;<span class="type">Int</span>=_*<span class="number">2</span></span><br><span class="line">d: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1213</span>/<span class="number">1757629874</span>@<span class="number">7</span>b95bdb0</span><br><span class="line"></span><br><span class="line"><span class="comment">//传递函数作参时使用一个_</span></span><br><span class="line">scala&gt; op(<span class="string">"hello"</span>,_.reverse)</span><br><span class="line">res4: <span class="type">String</span> = olleh</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用多个_ ,注意因为每个参数只能使用一次,所以三个_按参数顺序对应了x,y,z</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>,c:<span class="type">Int</span>,f:(<span class="type">Int</span>,<span class="type">Int</span>,<span class="type">Int</span>)=&gt;<span class="type">Int</span>)=f(a,b,c)</span><br><span class="line">top: (a: <span class="type">Int</span>, b: <span class="type">Int</span>, c: <span class="type">Int</span>, f: (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; top(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,_+_*_)</span><br><span class="line">res5: <span class="type">Int</span> = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//总是,_都是在调用或函数体里用,函数定义还是要把标识符和类型说明噢</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="部分应用函数"><a href="#部分应用函数" class="headerlink" title="部分应用函数"></a>部分应用函数</h3><p>currying,简洁的用法是使用有多个参数列表的函数,部分应用(部分参数设置好的函数)赋值给变量的时候,使用_占位符表示一个参数列表.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">fof</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>)= y%x==<span class="number">0</span></span><br><span class="line">fof: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Boolean</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> iseven=fof(<span class="number">2</span>)_</span><br><span class="line">iseven: <span class="type">Int</span> =&gt; <span class="type">Boolean</span> = $$<span class="type">Lambda</span>$<span class="number">1225</span>/<span class="number">1920199</span>@<span class="number">7</span>d28cdcd</span><br><span class="line"></span><br><span class="line">scala&gt; iseven(<span class="number">32</span>)</span><br><span class="line">res6: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="偏函数"><a href="#偏函数" class="headerlink" title="偏函数"></a>偏函数</h3><p>有些函数不能支持输入类型所有的值,比如算根,你就不能输入负数.这种函数就叫偏函数,因为只能应用于部分数据.scala内对偏函数的支持是对输入应用一系列case+lambda.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sh:<span class="type">Int</span>=&gt;<span class="type">String</span> =&#123;</span><br><span class="line">     | <span class="keyword">case</span> <span class="number">200</span>=&gt;<span class="string">"okay"</span></span><br><span class="line">     | <span class="keyword">case</span> <span class="number">400</span>=&gt;<span class="string">"error"</span></span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; sh(<span class="number">200</span>)</span><br><span class="line">res7: <span class="type">String</span> = okay</span><br><span class="line"></span><br><span class="line">scala&gt; sh(<span class="number">1</span>)<span class="comment">//不符合case会报错.</span></span><br><span class="line">scala.<span class="type">MatchError</span>: <span class="number">1</span> (of <span class="class"><span class="keyword">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Integer</span>)</span></span><br><span class="line"><span class="class">  <span class="title">at</span> .<span class="title">$anonfun$sh$1</span>(<span class="params">&lt;console&gt;:11</span>)</span></span><br><span class="line"><span class="class">  <span class="title">at</span> .<span class="title">$anonfun$sh$1$adapted</span>(<span class="params">&lt;console&gt;:11</span>)</span></span><br><span class="line"><span class="class">  <span class="title">at</span> <span class="title">$$Lambda$1266/1617646499</span>.<span class="title">apply</span>(<span class="params"><span class="type">Unknown</span> <span class="type">Source</span></span>)</span></span><br><span class="line"><span class="class">  ... 30 <span class="title">elided</span></span></span><br></pre></td></tr></table></figure></p>
<h3 id="传名函数"><a href="#传名函数" class="headerlink" title="传名函数"></a>传名函数</h3><p>使用这个语法定义函数参数,同时支持使用常规的值/函数来作为参数<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span></span>(x: =&gt;<span class="type">Int</span>)=&#123;<span class="comment">//x是一个Int值或者产出Int的函数</span></span><br><span class="line">	x*<span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">double(<span class="number">2</span>)<span class="comment">//√</span></span><br><span class="line">double((a:<span class="type">Int</span>)=&gt;a*<span class="number">2</span>)<span class="comment">//√</span></span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/11/数据类型和表达式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/11/数据类型和表达式/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-11T16:23:33+08:00">
                2017-07-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据类型和表达式"><a href="#数据类型和表达式" class="headerlink" title="数据类型和表达式"></a>数据类型和表达式</h1><p>#scala</p>
<p>val定义:val <name>:<type>=<literal>(不可变值)</literal></type></name></p>
<pre><code>* 不需要有type指定,编译器可推导
</code></pre><p>var定义:var <name>:<type>=<literal> (可变值)</literal></type></name></p>
<pre><code>* 不要把var转化类型到不可转化,比如x=1,x=“hello”,数值型可以转.
</code></pre><blockquote>
<p>请优先使用val.  </p>
</blockquote>
<p>数值类型:</p>
<ol>
<li>Byte 1字节</li>
<li>Short 2字节</li>
<li>Int 4字节</li>
<li>Long 8字节</li>
<li>Float 4字节</li>
<li>Double 8字节<br>上面的是按照等级低到高排序,等级低类型可以转换到等级高类型,而不允许高到低(不会截取),以免丢失数据.但你可以使用toInt()来截取到Int型.</li>
</ol>
<p>String</p>
<ul>
<li>可以使用==比较值</li>
<li>可以使用+拼接</li>
<li>使用”””创建多行String.</li>
<li>内插: <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> item=apple</span><br><span class="line"><span class="string">s"How do you like them <span class="subst">$&#123;item&#125;</span>s"</span>  -&gt;how do you like them apples</span><br><span class="line">```注意开始的s和$引用符号</span><br><span class="line"></span><br><span class="line"><span class="type">Boolean</span>类型:<span class="literal">true</span>/<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">类型层次:</span><br><span class="line">![](%<span class="type">E6</span>%<span class="number">95</span>%<span class="type">B0</span>%<span class="type">E6</span>%<span class="number">8</span>D%<span class="type">AE</span>%<span class="type">E7</span>%<span class="type">B1</span>%<span class="type">BB</span>%<span class="type">E5</span>%<span class="number">9</span>E%<span class="number">8</span>B%<span class="type">E5</span>%<span class="number">92</span>%<span class="number">8</span>C%<span class="type">E8</span>%<span class="type">A1</span>%<span class="type">A8</span>%<span class="type">E8</span>%<span class="type">BE</span>%<span class="type">BE</span>%<span class="type">E5</span>%<span class="type">BC</span>%<span class="number">8</span>F/<span class="type">IMG_0965_600x450</span>.<span class="type">JPG</span>)</span><br><span class="line"><span class="type">Any</span>是所有类型的父类,<span class="type">AnyVal</span>是所有<span class="keyword">val</span>类型的根,<span class="type">AnyRef</span>是所有引用类型的根.<span class="type">Nothing</span>是所有类型的子类,<span class="type">Null</span>是所有指示<span class="literal">null</span>值的<span class="type">AnyRef</span>类型的子类(为了给<span class="literal">null</span>值提供一个类型而存在)</span><br><span class="line"></span><br><span class="line">常用操作符:</span><br><span class="line">* asInstanceOf[&lt;<span class="class"><span class="keyword">type</span><span class="title">&gt;</span>]</span>:强转</span><br><span class="line">* getClass:返回类型</span><br><span class="line">* isInstanceOf</span><br><span class="line">* hashcode</span><br><span class="line">* toXXX(<span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">tuple:</span><br><span class="line">可以包含n个元素,以()表示,或者二元组以-&gt;表示</span><br><span class="line">```scala</span><br><span class="line"><span class="keyword">val</span> info=(<span class="number">5</span>,<span class="string">"aaa"</span>,<span class="literal">true</span>)</span><br><span class="line">info._3<span class="comment">//访问true</span></span><br><span class="line"><span class="comment">//大小为2的元组:</span></span><br><span class="line"><span class="keyword">val</span> red=<span class="string">"red"</span>-&gt;<span class="string">"hahaha"</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>表达式:</p>
<ul>
<li>使用{}创建表达式块,最后一个表达式的值为返回值:<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> s=&#123;<span class="keyword">val</span> x=<span class="number">5</span>*<span class="number">20</span>;x+<span class="number">10</span>&#125;<span class="comment">//或者多行</span></span><br><span class="line"><span class="keyword">val</span> s=&#123;</span><br><span class="line">     | <span class="keyword">val</span> x=<span class="number">5</span>*<span class="number">20</span></span><br><span class="line">     | x+<span class="number">10</span></span><br><span class="line">     | &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>匹配表达式(match)<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> x=<span class="string">"mon"</span></span><br><span class="line"> <span class="keyword">val</span> max= x <span class="keyword">match</span>&#123;</span><br><span class="line">     | <span class="keyword">case</span> <span class="string">"mon"</span> | <span class="string">"tru"</span> | <span class="string">"haha"</span>  =&gt;<span class="string">"ok"</span></span><br><span class="line">     | <span class="keyword">case</span> <span class="string">"SAT"</span> <span class="keyword">if</span> a&gt;<span class="number">5</span> =&gt;println(<span class="string">"not ok"</span>)</span><br><span class="line">		|					<span class="string">"ERROR"</span><span class="comment">//打印并且返回最后一个表达式值"ERROR"</span></span><br><span class="line">		| <span class="keyword">case</span> other =&gt;println(<span class="string">s"<span class="subst">$other</span>"</span>)<span class="comment">//把变量值绑定到other并在case内使用</span></span><br><span class="line">     | &#125;<span class="comment">//关注每种case的不同操作和多种情况共用一个case</span></span><br><span class="line">			<span class="comment">//关注case 后的 if 哨兵</span></span><br><span class="line">			<span class="comment">//匹配不上的时候会抛出MatchError</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//匹配类型</span></span><br><span class="line"><span class="keyword">val</span> x:<span class="type">Int</span> =<span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> y:<span class="type">Any</span> =x</span><br><span class="line"></span><br><span class="line">y <span class="keyword">match</span>&#123;</span><br><span class="line">	| <span class="keyword">case</span> x:<span class="type">String</span> =&gt; xxx</span><br><span class="line">	| <span class="keyword">case</span> x:<span class="type">Int</span> =&gt; xxx</span><br></pre></td></tr></table></figure></p>
<p>循环</p>
<blockquote>
<p>for ( <identifier> &lt;- <iterator>  [if xxx] [yield]) {<expressions>} )<br>有yield的话,每次循环表达式里所有值作为一个collection返回,不然不能访问表达式的值;可以有<strong>if哨兵</strong><br>iterator示例:</expressions></iterator></identifier></p>
<ul>
<li>1 to 7</li>
<li>a Seq</li>
<li>…<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> power=<span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">8</span> y &lt;- <span class="number">1</span> to <span class="number">4</span>; pow=i*y) <span class="keyword">yield</span> pow</span><br><span class="line"><span class="comment">//注意嵌套迭代和值绑定</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Bulk Load/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/07/Bulk Load/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-07T21:10:37+08:00">
                2017-07-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Bulk-Load"><a href="#Bulk-Load" class="headerlink" title="Bulk Load"></a>Bulk Load</h1><p>#推荐引擎</p>
<p>#hbase</p>
<p>BulkLoad的使用场景：</p>
<p>Use Cases for BulkLoad:</p>
<p>1.Loading your original dataset into HBase for the first time - Your initial dataset might be quite large, and bypassing the HBase write path can speed up the process considerably.</p>
<p>2.Incremental Load - To load new data periodically, use BulkLoad to import it in batches at your preferred intervals. This alleviates latency problems and helps you to achieve service-level agreements (SLAs). However, one trigger for compaction is the number of HFiles on a RegionServer. Therefore, importing a large number of HFiles at frequent intervals can cause major compactions to happen more often than they otherwise would, negatively impacting performance. You can mitigate this by tuning the compaction settings such that the maximum number of HFiles that can be present without triggering a compaction is very high, and relying on other factors, such as the size of the Memstore, to trigger compactions.</p>
<p>3.Data needs to originate elsewhere - If an existing system is capturing the data you want to have in HBase and needs to remain active for business reasons, you can periodically BulkLoad data from the system into HBase so that you can perform operations on it without impacting the system.<br>从上面看出来bulkload不是只能在初始的时候进行一次加载的，如果你的业务是每天定时更新数据，每天bulkload也是挺好的方案。</p>
<p>BulkLoad的原理和流程：</p>
<p>根据HDFS上的数据或者外部的数据生成Hbase的底层Hfile数据。<br>根据生成的目标Hfile，利用Hbase提供的BulkLoad工具将Hfile Load到Hbase目录下面。</p>
<p><img src="/2017/07/07/Bulk Load/2177145-93a666959e6f5a48.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Hive整合HBase/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/07/Hive整合HBase/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-07T19:34:54+08:00">
                2017-07-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Hive整合HBase"><a href="#Hive整合HBase" class="headerlink" title="Hive整合HBase"></a>Hive整合HBase</h1><p>#推荐引擎</p>
<p>#hbase</p>
<blockquote>
<p>Hive 1.x will remain compatible with HBase 0.98.x and lower versions. Hive 2.x will be compatible with HBase 1.x and higher. (See HIVE-10990 for details.) Consumers wanting to work with HBase 1.x using Hive 1.x will need to compile Hive 1.x stream code themselves.现在我自己使用hbase1.3.1/hive2.1.1  </p>
</blockquote>
<p>采用的是hive-hbase-handler方式,采用的是hive的内部-非本地表,外部-非本地表.</p>
<ul>
<li>内非:创建表使用create table且有stored by条件.hive将定义储存在元数据store中,通过handler生成相应对象</li>
<li>外非:使用create external table且有stored by语句.hive在元数据store中注册相应信息,通过handler检查和其他系统是否匹配.</li>
</ul>
<p>总的来讲,这是一种成熟的基于Hbase的sql解决方案.但是一个最大的缺点就是”查询速度慢”</p>
<p>使用模式</p>
<ol>
<li>上传文件到HDFS,Hive创建外部表,导入数据.</li>
<li>创建内部非本地表.实际上是在Hbase里建表了.<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx(a <span class="keyword">string</span>,b <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHanlder'</span></span><br><span class="line"><span class="keyword">with</span> SerdeProperties(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,cf1:score"</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase,table.name"</span>=<span class="string">'hbase_xx'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="/2017/07/07/Hive整合HBase/IMG_0910_600x450.JPG" alt=""><br>columns.mapping需要N个声明,与hive表字段一致,声明可以使:key(仅一个),cf:cname多个,按顺序排.</p>
<ol>
<li>把数据导入这个Hbase表<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> hbase_table <span class="keyword">select</span> <span class="keyword">name</span>,score <span class="keyword">from</span> hive_table</span><br><span class="line"><span class="string">``</span><span class="string">` </span></span><br><span class="line"><span class="string">4. 使用HQL执行查询</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">还可以创建外部非本地表,适合于Hbase表已经存在,语法一般仅仅在create external table处不同.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 多列/多列族映射</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`</span><span class="string">``</span><span class="keyword">sql</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx(a <span class="keyword">string</span>,b <span class="built_in">int</span>,c <span class="keyword">string</span>,d <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHanlder'</span></span><br><span class="line"><span class="keyword">with</span> SerdeProperties</span><br><span class="line">(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,cf1:b,cf1:c,cf2:d"</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase,table.name"</span>=<span class="string">'hbase_xx'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="HiveMap"><a href="#HiveMap" class="headerlink" title="HiveMap"></a>HiveMap</h3><p>访问整个列族,不需要指定列名.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx(a <span class="keyword">string</span>,b <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHanlder'</span></span><br><span class="line"><span class="keyword">with</span> SerdeProperties(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,cf:"</span>)</span><br></pre></td></tr></table></figure></p>
<p>上面cf列族就包含了map,两个列,一个是string,一个int.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> xxx <span class="keyword">select</span> sname,<span class="keyword">map</span>(sname,score) <span class="keyword">from</span> hive_tb</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Hbase书/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/07/Hbase书/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-07T11:10:50+08:00">
                2017-07-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Hbase书"><a href="#Hbase书" class="headerlink" title="Hbase书"></a>Hbase书</h1><p>#推荐引擎</p>
<p>#hbase</p>
<p>管理界面端口:60010</p>
<h3 id="储存模型和增删改查机制"><a href="#储存模型和增删改查机制" class="headerlink" title="储存模型和增删改查机制"></a>储存模型和增删改查机制</h3><p>一个HRegionServer包含多个HRegion,每个HRegion对应了table中的一个Region(包含key从a~b的一系列row,管理整行),Region又由多个HStore(管理列族)组成,每个<strong>HStore对应table中一个列族</strong>的储存.</p>
<p>HStore包含MemStore,BlockCache和StoreFile(Hfile).<br>首先用户写入数据在MemStore中,当大小超过一个threshold1之后形成一个Hfile,然后MemStore重新积累,不断形成Hfile,当Htore中Hfile数量超过threshold2之后,触发compact把多个Hfile合并成一个. 当合并大小又超过threshold3,会触发split把当前Region分裂成两个并分配到新的HRegionServer,以分流压力.</p>
<p>Hbase都是以字节数组写数据,所以需要’’.toBytes()</p>
<p>为了避免menstore的数据没有flush到硬盘就丢失了,每一台Hbase服务器维护一个WAL来记录发生的变化,WAL是一个文件.知道WAL新纪录成功写入,写才算完成.当server崩溃,HBASE可以自动回放WAL来恢复</p>
<p>Delete删除背后,并不马上删除内容而是打上删除标记,当执行一次major compaction的时候,那些标记要删除的才会被删除.合并分为小合并,大合并;小合并把多个小Hfile合并成一个大Hfile,这是为了读一个完整的行的时候能少弄一点Hfile IO.大合并把指定Region内一个列族(Hstore)的所有Hfile.</p>
<p>对于时间版本,每次对cell操作都会存一个新时间版本,取的时候可以根据参数取出不同的时间版本,默认储存3个版本.</p>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>Hbase采用四维数据坐标:{rowKey,cf,column,timestamp}确定一个值,看做一个map,key是四维坐标,并且按rowKey字典序排序<br><img src="/2017/07/07/Hbase书/FullSizeRender.jpg" alt=""><br>取数据的时候,你可以只给某前几个维的坐标,它会返回余下的所有部分.</p>
<p>Hbase设计上没有数据类型,数据记录(可能)包含不一致的列结构,这是<strong>半结构化数据</strong></p>
<p>逻辑模型上,看做sorted map of maps.从java上可以看做下面,用一个图表示就是:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;Rowkey,Map&lt;ColumnFamily,Map&lt;Qualifier,Map&lt;Version,Data&gt;&gt;&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>
<p><img src="/2017/07/07/Hbase书/FullSizeRender%202_600x450.jpg" alt=""></p>
<p>物理模型上,是面向列族的,表按列族分组.每个列族有自己的Hfile集合.Hfile里比如一行数据,储存的结构如下:<br><img src="/2017/07/07/Hbase书/FullSizeRender%203_600x113.jpg" alt=""><br>也就是说,这行数据里每一列的数据在物理上存成一行.这也意味着,读取的时候,物理上只需要按需读取一些列族的数据,这有利于稀疏数据集合的快速读取.</p>
<h3 id="表扫描"><a href="#表扫描" class="headerlink" title="表扫描"></a>表扫描</h3><p>查找包含某个特定值的唯一方法是:使用扫描命令读出表的某些部分,然后使用过滤器来得到有关记录.当然,你得到的东西也是sorted的.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//放入数据</span></span><br><span class="line">put=<span class="keyword">new</span> Put(<span class="string">"RowKey"</span>);</span><br><span class="line">put.add(Bytes.toByte(cf),Bytes.toByte(user),Bytes.toByte(<span class="string">"pw"</span>))<span class="comment">//这就添加了一列数据到行,user数据</span></span><br><span class="line">put.add(Bytes.toByte(cf),Bytes.toByte(twit),Bytes.toByte(<span class="string">"fuck"</span>))<span class="comment">//这添加了这一行twit列的数据.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//扫描数据</span></span><br><span class="line">Scan s=<span class="keyword">new</span> Scan(StartRow,EndROw)<span class="comment">//Roekey是按字典序排的,所以要设计好Rowkey尽量取出来是你想要的.</span></span><br><span class="line">s.addColumn(<span class="string">"cf"</span>,<span class="string">"qualifier"</span>)<span class="comment">//取出这些row数据的某一列</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//过滤,是在server端过滤,只返回对应结果,而不是返回之后再过滤</span></span><br><span class="line">Filter f=...</span><br><span class="line">s.setFilter(f);</span><br><span class="line"><span class="comment">//比如使用ValueFilter</span></span><br><span class="line">f=<span class="keyword">new</span> ValueFilter(CompareOp.equal,<span class="keyword">new</span> RegexStringComparator(<span class="string">'regex'</span>));</span><br></pre></td></tr></table></figure>
<p>过滤器可以应用到rowKey,qualifier或者value,所以你就充分地过滤咯..</p>
<h2 id="HBase与并行计算-以MapRed为例"><a href="#HBase与并行计算-以MapRed为例" class="headerlink" title="HBase与并行计算(以MapRed为例)"></a>HBase与并行计算(以MapRed为例)</h2><ol>
<li>作为数据源</li>
<li>接收数据</li>
</ol>
<p>使用的是一套Mapred的API.</p>
<h2 id="HBase表设计"><a href="#HBase表设计" class="headerlink" title="HBase表设计"></a>HBase表设计</h2><p>RowKey上:</p>
<ol>
<li>字典序从大到小,比如你想展现出一个排行榜,那么对于这种排序需求你需要定义好RowKey让其取出来就有序</li>
<li>保证散列,使得数据不至于都在一个Region上以降低查询负载.比如储存视频网站的观影记录,RowKey设计为userid+videoId,这样的话,查一个User的时候就可能造成负载很大.可以:<ul>
<li>反转User_id</li>
<li>散列USer_id</li>
</ul>
</li>
<li>RowKey尽量短,提高效率,时间尽量用Long表示,尽量使用编码压缩.</li>
</ol>
<p>列族设计,列组要尽量少一点.看几个实例吧.<br><img src="/2017/07/07/Hbase书/IMG_0903_600x800.JPG" alt=""><br><img src="/2017/07/07/Hbase书/IMG_0904_600x800.JPG" alt=""><br><img src="/2017/07/07/Hbase书/IMG_0905_600x800.JPG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/使用SparkSQLDataFrame读取HBase表/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/06/使用SparkSQLDataFrame读取HBase表/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-06T20:27:07+08:00">
                2017-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="使用SparkSQL-DataFrame读取HBase表"><a href="#使用SparkSQL-DataFrame读取HBase表" class="headerlink" title="使用SparkSQL/DataFrame读取HBase表"></a>使用SparkSQL/DataFrame读取HBase表</h1><p>#推荐引擎</p>
<p>#hbase</p>
<p>HBaseContext类：<br><a href="https://github.com/apache/hbase/tree/master/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark" target="_blank" rel="noopener">https://github.com/apache/hbase/tree/master/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark</a></p>
<p>HBaseTableCatalog类： <a href="https://github.com/apache/hbase/tree/master/hbase-spark/src/main/scala/org/apache/spark/sql/datasources/hbase" target="_blank" rel="noopener">https://github.com/apache/hbase/tree/master/hbase-spark/src/main/scala/org/apache/spark/sql/datasources/hbase</a></p>
<p>这两个类都是在hbase-spark模块中的，这个jar包下载地址： <a href="http://maven.wso2.org/nexus/content/repositories/Apache/org/apache/hbase/hbase-spark/2.0.0-SNAPSHOT/" target="_blank" rel="noopener">http://maven.wso2.org/nexus/content/repositories/Apache/org/apache/hbase/hbase-spark/2.0.0-SNAPSHOT/</a></p>
<p>HBase-Spark Connector(在HBase-Spark 模块中）利用了在Spark-1.2.0中引入的DataSource API(SPARK-3247)，在简单的HBase KV存储和复杂的关系型SQL查询之间架起了桥梁，使得用户可以在HBase上使用Spark执行复杂的数据分析工作。HBase Dataframe是一个标准的Spark Dataframe，能够与任何其他的数据源进行交互，比如Hive,Orc，Parquet,JSON等。HBase-Spark　Connector应用了关键技术，如分区剪枝（partition pruning)，列剪枝(column pruning)，谓詞下推(predicate pushdown)和数据局部性（data locality）。</p>
<p>要使用HBase-Spark Connector，用户需要定义在HBase和Spark表之间的映射关系的schema目录，准备数据，并且填充到HBase表中，然后加载HBase Dataframe。之后，用户可以使用SQL查询做集成查询和访问记录HBase的表。以下描述了这个的基本步骤：</p>
<p>１、定义目录（Define catalog） ２、保存DataFrame ３、加载DataFrame ４、SQL 查询</p>
<h2 id="定义目录（Define-catalog）"><a href="#定义目录（Define-catalog）" class="headerlink" title="定义目录（Define catalog）"></a>定义目录（Define catalog）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def catalog = s&quot;&quot;&quot;&#123; |&quot;table&quot;:&#123;&quot;namespace&quot;:&quot;default&quot;, &quot;name&quot;:&quot;table1&quot;&#125;, |&quot;rowkey&quot;:&quot;key&quot;, |&quot;columns&quot;:&#123; |&quot;col0&quot;:&#123;&quot;cf&quot;:&quot;rowkey&quot;, &quot;col&quot;:&quot;key&quot;, &quot;type&quot;:&quot;string&quot;&#125;, |&quot;col1&quot;:&#123;&quot;cf&quot;:&quot;cf1&quot;, &quot;col&quot;:&quot;col1&quot;, &quot;type&quot;:&quot;string&quot;&#125; |&#125; |&#125;&quot;&quot;&quot;.stripMargin</span><br></pre></td></tr></table></figure>
<h2 id="定义类"><a href="#定义类" class="headerlink" title="定义类"></a>定义类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class HBaseRecord( col0: String, col1: String)</span><br></pre></td></tr></table></figure>
<h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = (0 to 255).map &#123; i =&gt; HBaseRecord(i.toString, &quot;extra&quot;)&#125; sc.parallelize(data).toDF.write.options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; &quot;5&quot;)).format(&quot;org.apache.hadoop.hbase.spark&quot;).save()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;console&gt;:34: error: not found: value HBaseTableCatalog sc.parallelize(data).toDF.write.options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; &quot;5&quot;)).format(&quot;org.apache.hadoop.hbase.spark&quot;).save()</span><br></pre></td></tr></table></figure>
<p>出现以上错误，解决方法为导入相应包，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import org.apache.spark.sql.datasources.hbase.&#123;HBaseTableCatalog&#125;</span><br><span class="line">import org.apache.spark.sql.datasources.hbase.HBaseTableCatalog</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(data).toDF.write.options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; &quot;5&quot;)).format(&quot;org.apache.hadoop.hbase.spark &quot;).save()</span><br><span class="line">&lt;console&gt;:29: error: value toDF is not a member of org.apache.spark.rdd.RDD[HBaseRecord] sc.parallelize(data).toDF.write.options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; &quot;5&quot;)).format(&quot;org.apache.hadoop.hbase.spark &quot;).save()</span><br></pre></td></tr></table></figure>
<p>報错了，说toDF不是org.apache.spark.rdd.RDD[HBaseRecord]的成员。之所以出现这个问题，是因为之前启动Spark-shell的时候，sqlContext不能使用[1]，没有创建成功，因为我的集群的没有启动，所以，解决方法就是启动整个集群，然后重新进入spark-shell。当然，在启动的时候，要加上一个hbase-spark包到classpath，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop@master:~$ spark-1.6.0-bin-hadoop2.4/bin/spark-shell --jars /home/yang/Downloads/hbase-spark-2.0.0-20160316.173537-2.jar</span><br></pre></td></tr></table></figure>
<p>因为我们有用到HBaseTableCatalog类，这个类是在hbase-spark包里的，所以，我们要引入这个包。</p>
<p>继续执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(data).toDF.write.options(Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; &quot;5&quot;)).format(&quot;org.apache.hadoop.hbase.spark&quot;).save()</span><br><span class="line">java.lang.NullPointerException at org.apache.hadoop.hbase.spark.HBaseRelation.&lt;init&gt;(DefaultSource.scala:125) at org.apache.hadoop.hbase.spark.DefaultSource.createRelation(DefaultSource.scala:74) at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:36) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:41) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:43) at $iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:45) at $iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:47) at $iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:49) at $iwC$$iwC.&lt;init&gt;(&lt;console&gt;:51) at $iwC.&lt;init&gt;(&lt;console&gt;:53) at &lt;init&gt;(&lt;console&gt;:55) at .&lt;init&gt;(&lt;console&gt;:59) at .&lt;clinit&gt;(&lt;console&gt;) at .&lt;init&gt;(&lt;console&gt;:7) at .&lt;clinit&gt;(&lt;console&gt;) at $print(&lt;console&gt;) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br></pre></td></tr></table></figure>
<p>出现上述问题的原因是，由于没有初始化HBaseContext对象，所以，只有创建一个HBaseContext对象就好了。但注意，由于在spark-shell中缺少相应的hbase依赖包，所以，在此，你想创建HBaseContext包时，需要引入相应包，比较麻烦，建议在ＩＤＥ里测试这个程序，比如Scala IDE,或Intellij。</p>
<p>以下是完整程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.Path</span><br><span class="line">import org.apache.hadoop.hbase.&#123; HBaseConfiguration, HColumnDescriptor, HTableDescriptor &#125;</span><br><span class="line">import org.apache.hadoop.hbase.client.&#123; HBaseAdmin, HTable, Put &#125;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableInputFormat</span><br><span class="line">import org.apache.hadoop.hbase.spark.HBaseContext</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql._</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.sql.datasources.hbase._</span><br><span class="line">import org.apache.hadoop.hbase.spark.datasources.HBaseScanPartition</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes case class HBaseRecord( col0: String, col1: Int) object HBaseRecord &#123; def apply(i: Int, t: Int): HBaseRecord = &#123; val s = s&quot;&quot;&quot;row$&#123;&quot;%03d&quot;.format(i)&#125;&quot;&quot;&quot; HBaseRecord(s, i) &#125;</span><br><span class="line">&#125; object Test &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;test spark sql&quot;); conf.setMaster(&quot;spark://master:7077&quot;); val sc = new SparkContext(&quot;local&quot;, &quot;test&quot;) val config = HBaseConfiguration.create() val hbaseContext = new HBaseContext(sc, config, null) def catalog = s&quot;&quot;&quot;&#123; |&quot;table&quot;:&#123;&quot;namespace&quot;:&quot;default&quot;, &quot;name&quot;:&quot;table4&quot;&#125;, |&quot;rowkey&quot;:&quot;key&quot;, |&quot;columns&quot;:&#123; |&quot;col0&quot;:&#123;&quot;cf&quot;:&quot;rowkey&quot;, &quot;col&quot;:&quot;key&quot;, &quot;type&quot;:&quot;string&quot;&#125;, |&quot;col1&quot;:&#123;&quot;cf&quot;:&quot;cf1&quot;, &quot;col&quot;:&quot;col1&quot;, &quot;type&quot;:&quot;int&quot;&#125; |&#125; |&#125;&quot;&quot;&quot;.stripMargin val sqlContext = new SQLContext(sc); import sqlContext.implicits._ def withCatalog(cat: String): DataFrame = &#123; sqlContext .read .options(Map(HBaseTableCatalog.tableCatalog -&gt; cat)) .format(&quot;org.apache.hadoop.hbase.spark&quot;) .load() &#125; val df = withCatalog(catalog) val res = df.select(&quot;col1&quot;) res.show() df.registerTempTable(&quot;table4&quot;) sqlContext.sql(&quot;select count(col0),sum(col1) from table4 where col1&gt;&apos;20&apos; and col1&lt;&apos;26&apos; &quot;).show println(&quot;-----------------------------------------------------&quot;); sqlContext.sql(&quot;select count(col1),avg(col1) from table4&quot;).show &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/HBase and Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/06/HBase and Spark/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-06T18:53:12+08:00">
                2017-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="HBase-and-Spark"><a href="#HBase-and-Spark" class="headerlink" title="HBase and Spark"></a>HBase and Spark</h1><p>#推荐引擎</p>
<p>#hbase</p>
<p>中文版:<a href="bear://x-callback-url/open-note?id=02FCACE4-C961-4748-96AB-E63683706760-4667-0000658B4F6D95AD" target="_blank" rel="noopener">使用SparkSQL/DataFrame读取HBase表</a></p>
<h3 id="basic-spark"><a href="#basic-spark" class="headerlink" title="basic spark"></a>basic spark</h3><p>At the root of all Spark and HBase integration is th<strong>e HBaseContext</strong>. The HBaseContext takes in HBase configurations and <strong>pushes them to the Spark executors</strong>. This allows us to have an HBase Connection per Spark Executor in a static location</p>
<p>Think of every Spark Executor as a multi-threaded client application. This allows any Spark Tasks running on the executors to access the shared Connection object.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  List&lt;<span class="keyword">byte</span>[]&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  list.add(Bytes.toBytes(<span class="string">"1"</span>));</span><br><span class="line">  ...</span><br><span class="line">  list.add(Bytes.toBytes(<span class="string">"5"</span>));</span><br><span class="line">  JavaRDD&lt;<span class="keyword">byte</span>[]&gt; rdd = jsc.parallelize(list);<span class="comment">//创建了一个RDD</span></span><br><span class="line"></span><br><span class="line">  Configuration conf = HBaseConfiguration.create();</span><br><span class="line">  JavaHBaseContext hbaseContext = <span class="keyword">new</span> JavaHBaseContext(jsc, conf);<span class="comment">//创建HBASEContext</span></span><br><span class="line"></span><br><span class="line">  hbaseContext.foreachPartition(rdd,</span><br><span class="line">      <span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;Iterator&lt;<span class="keyword">byte</span>[]&gt;, Connection&gt;&gt;() &#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;Iterator&lt;<span class="keyword">byte</span>[]&gt;, Connection&gt; t)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Table table = t._2().getTable(TableName.valueOf(tableName));<span class="comment">//使用connection获取表</span></span><br><span class="line">    BufferedMutator mutator = t._2().getBufferedMutator(TableName.valueOf(tableName));<span class="comment">//获取mutator</span></span><br><span class="line">    <span class="keyword">while</span> (t._1().hasNext()) &#123;</span><br><span class="line">      <span class="keyword">byte</span>[] b = t._1().next();</span><br><span class="line">      Result r = table.get(<span class="keyword">new</span> Get(b));<span class="comment">//从table get t_1</span></span><br><span class="line">      <span class="keyword">if</span> (r.getExists()) &#123;</span><br><span class="line">       mutator.mutate(<span class="keyword">new</span> Put(b));<span class="comment">//改变,进行put..</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    mutator.flush();</span><br><span class="line">    mutator.close();</span><br><span class="line">    table.close();<span class="comment">//应用更改</span></span><br><span class="line">   &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  jsc.stop();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//就是一些增删改查api</span></span><br></pre></td></tr></table></figure>
<p>The examples above illustrate how to do a foreachPartition with a connection. notice there is<br>a hBaseRDD() function.</p>
<h3 id="bulk-load"><a href="#bulk-load" class="headerlink" title="bulk load"></a>bulk load</h3><p>There are <strong>two options</strong> for bulk loading data into HBase with Spark. There is the <strong>basic bulk load</strong> functionality that will work for cases where your rows have <strong>millions of columns</strong> and cases where your columns are not consolidated and partitions before the on the map side of the Spark bulk load process.</p>
<p>There is also a <strong>thin record bulk load option</strong> with Spark, this second option is designed for tables that have <strong>less then 10k columns per row</strong>. The advantage of this second option is <strong>higher throughput</strong> and less over all load on the Spark shuffle operation.</p>
<p>In Spark terms, the bulk load will be implemented around a the Spark repartitionAndSortWithinPartitions followed by a Spark foreachPartition.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stagingFolder = ...</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">      (<span class="type">Bytes</span>.toBytes(<span class="string">"1"</span>),</span><br><span class="line">        (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"a"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"foo1"</span>))),</span><br><span class="line">      (<span class="type">Bytes</span>.toBytes(<span class="string">"3"</span>),</span><br><span class="line">        (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"b"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"foo2.b"</span>))), ...</span><br><span class="line"><span class="comment">//创建rdd</span></span><br><span class="line"></span><br><span class="line">rdd.hbaseBulkLoad(<span class="type">TableName</span>.valueOf(tableName),<span class="comment">//把rdd转化为一个hbase表的格式</span></span><br><span class="line">  t =&gt; &#123;</span><br><span class="line">   <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">   <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = t._2(<span class="number">0</span>)._1</span><br><span class="line">   <span class="keyword">val</span> qualifier = t._2(<span class="number">0</span>)._2</span><br><span class="line">   <span class="keyword">val</span> value = t._2(<span class="number">0</span>)._3</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> keyFamilyQualifier= <span class="keyword">new</span> <span class="type">KeyFamilyQualifier</span>(rowKey, family, qualifier)</span><br><span class="line"></span><br><span class="line">   <span class="type">Seq</span>((keyFamilyQualifier, value)).iterator</span><br><span class="line">  &#125;,</span><br><span class="line">  stagingFolder.getPath)<span class="comment">//其实是创建了Hfiles</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(config)<span class="comment">//do bulkload</span></span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingFolder.getPath),</span><br><span class="line">  conn.getAdmin, table, conn.getRegionLocator(<span class="type">TableName</span>.valueOf(tableName)))</span><br></pre></td></tr></table></figure>
<p>The hbaseBulkLoad function takes three required parameters:</p>
<ol>
<li>table name</li>
<li>A function that will convert a record in the RDD to a tuple key value par. With the tuple key being a KeyFamilyQualifer object and the value being the cell value. The KeyFamilyQualifer object will hold the RowKey, Column Family, and Column Qualifier. The shuffle will partition on the RowKey but will sort by all three values.一个转换函数</li>
<li>The temporary path for the HFile to be written out too</li>
</ol>
<p>Following the Spark bulk load command, use the HBase’s <strong>LoadIncrementalHFiles</strong> object to load the newly created HFiles into HBase.</p>
<p>下面是thin function的例子:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stagingFolder = ...</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">      (<span class="string">"1"</span>,</span><br><span class="line">        (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"a"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"foo1"</span>))),</span><br><span class="line">      (<span class="string">"3"</span>,</span><br><span class="line">        (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"b"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"foo2.b"</span>))), ...</span><br><span class="line"></span><br><span class="line">rdd.hbaseBulkLoadThinRows(hbaseContext,</span><br><span class="line">      <span class="type">TableName</span>.valueOf(tableName),</span><br><span class="line">      t =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> rowKey = t._1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> familyQualifiersValues = <span class="keyword">new</span> <span class="type">FamiliesQualifiersValues</span></span><br><span class="line">        t._2.foreach(f =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = f._1</span><br><span class="line">          <span class="keyword">val</span> qualifier = f._2</span><br><span class="line">          <span class="keyword">val</span> value:<span class="type">Array</span>[<span class="type">Byte</span>] = f._3</span><br><span class="line"></span><br><span class="line">          familyQualifiersValues +=(family, qualifier, value)</span><br><span class="line">        &#125;)</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">ByteArrayWrapper</span>(<span class="type">Bytes</span>.toBytes(rowKey)), familyQualifiersValues)</span><br><span class="line">      &#125;,</span><br><span class="line">      stagingFolder.getPath,</span><br><span class="line">      <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">FamilyHFileWriteOptions</span>],</span><br><span class="line">      compactionExclude = <span class="literal">false</span>,</span><br><span class="line">      <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(config)</span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingFolder.getPath),</span><br><span class="line">  conn.getAdmin, table, conn.getRegionLocator(<span class="type">TableName</span>.valueOf(tableName)))</span><br></pre></td></tr></table></figure></p>
<p>不同就在于hbaseBulkLoadThinRows()方法,它 returns a tuple with the <strong>first value being the row key</strong> and the <strong>second value being an object of FamiliesQualifiersValues</strong>, which will contain all the values for this row for all column families.</p>
<h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><p>HBase-Spark Connector leverages DataSource API, bridges the gap between simple HBase KV store and complex relational SQL queries and enables users to perform complex data analytical work on top of HBase using Spark. </p>
<p>HBase Dataframe is a standard Spark Dataframe, and is able to interact with any other data sources such as Hive. To use <strong>HBase-Spark connector</strong>, users need to define the Catalog for the schema mapping between HBase and Spark tables, prepare the data and populate the HBase table, then load HBase DataFrame. After that, users can do integrated query and access records in HBase table with SQL query</p>
<p>save the dataframe:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseRecord</span>(<span class="params">//定义数据模板</span></span></span><br><span class="line"><span class="class"><span class="params">   col0: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col1: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col2: <span class="type">Double</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col3: <span class="type">Float</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col4: <span class="type">Int</span>,       </span></span></span><br><span class="line"><span class="class"><span class="params">   col5: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col6: <span class="type">Short</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col7: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">   col8: <span class="type">Byte</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">HBaseRecord</span></span></span><br><span class="line"><span class="class"></span>&#123;                                                                                                             </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(i: <span class="type">Int</span>, t: <span class="type">String</span>): <span class="type">HBaseRecord</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> s = <span class="string">s""</span><span class="string">"row$&#123;"</span>%<span class="number">03</span><span class="string">d".format(i)&#125;"</span><span class="string">""</span>       </span><br><span class="line">      <span class="type">HBaseRecord</span>(s,</span><br><span class="line">      i % <span class="number">2</span> == <span class="number">0</span>,</span><br><span class="line">      i.toDouble,</span><br><span class="line">      i.toFloat,  </span><br><span class="line">      i,</span><br><span class="line">      i.toLong,</span><br><span class="line">      i.toShort,  </span><br><span class="line">      <span class="string">s"String<span class="subst">$i</span>: <span class="subst">$t</span>"</span>,      </span><br><span class="line">      i.toByte)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> data = (<span class="number">0</span> to <span class="number">255</span>).map &#123; i =&gt;  <span class="type">HBaseRecord</span>(i, <span class="string">"extra"</span>)&#125;<span class="comment">//创建了数据</span></span><br><span class="line"></span><br><span class="line">sc.parallelize(data).toDF.write.options(</span><br><span class="line"> <span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog, <span class="type">HBaseTableCatalog</span>.newTable -&gt; <span class="string">"5"</span>))</span><br><span class="line"> .format(<span class="string">"org.apache.hadoop.hbase.spark "</span>)</span><br><span class="line"> .save()</span><br><span class="line"></span><br><span class="line"><span class="comment">//把data弄成dataframe,write function returns a DataFrameWriter used to write the DataFrame to external storage systems ,Given a DataFrame with specified schema catalog, save function will create an HBase table with 5 regions and save the DataFrame inside.注意他这里说是会创建hbase table的,所以我们有了dataframe,我们就可以用这个来save喽</span></span><br></pre></td></tr></table></figure></p>
<p>Load the dataframe<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withCatalog</span></span>(cat: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  sqlContext</span><br><span class="line">  .read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;cat))</span><br><span class="line">  .format(<span class="string">"org.apache.hadoop.hbase.spark"</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br></pre></td></tr></table></figure></p>
<p>In ‘withCatalog’ function, sqlContext is a variable of SQLContext, which is the entry point for working with structured data (rows and columns) in Spark. read returns a DataFrameReader that can be used to read data in as a DataFrame. option function adds input options for the underlying data source to the DataFrameReader, and format function specifies the input data source format for the DataFrameReader. The load() function loads input in as a DataFrame. The date frame df returned by withCatalog function could be used to access HBase table.</p>
<p>注意,这都需要我们定义好cataLog:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">       |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span>table1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">       |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">       |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">         |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">       |&#125;</span></span><br><span class="line"><span class="string">     |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure></p>
<p>得到dataframe之后,可以把它注册为Temptable,然后在上面运行sql方法就可以做数据操作…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/17/Tuning Guide/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/17/Tuning Guide/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-17T16:36:04+08:00">
                2017-04-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Tuning-Guide"><a href="#Tuning-Guide" class="headerlink" title="Tuning Guide"></a>Tuning Guide</h1><p>#spark</p>
<p>#推荐引擎</p>
<p>Spark中的cache:<br>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间,而cache和persist的区别是：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。<br><img src="/2017/04/17/Tuning Guide/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-08%2009.52.05_800x503.png" alt=""><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>怎么做根据名字还是好分辨的,每种level的构造参数都不同,_2代表复制保存两份.每个参数具体的意思在上面:</p>
<ul>
<li>useDisk：使用硬盘（外存）</li>
<li>useMemory：使用内存</li>
<li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li>
<li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li>
<li>replication：备份数（在多个节点上备份）</li>
</ul>
<p>Most often, if the data fits in memory, <strong>the bottleneck is network bandwidth</strong>, but sometimes, you also need to do some tuning, such as storing RDDs in serialized form, to decrease memory usage. This guide will cover two main topics: </p>
<ol>
<li>data serialization, 既能加速网络性能又能减少内存使用;</li>
<li>memory tuning.</li>
</ol>
<p>对于同一个Application，它在一个worker上只能拥有一个executor</p>
<h3 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h3><p>有两个序列化库可以使用</p>
<ul>
<li>Java serialization: <strong>By default</strong>, Spark serializes objects using Java’s ObjectOutputStream framework, 只要实现的类实现了 java.io.Serializable即可. You can also control the performance of your serialization more closely by extending java.io.Externalizable.这个库很灵活,但速度不很快,and leads to large serialized formats for many classes.</li>
<li>Kryo serialization: Spark can also use the Kryo library (version 2) to serialize objects <strong>more quickly</strong>. Kryo is significantly faster and more compact than Java serialization,但不支持所有Serializable类型,并且需要提前注册你在程序中使用的类.</li>
</ul>
<p>你可以使用Kryo库,只需要calling conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”). This setting configures the serializer used for not only <strong>shuffling data between worker nodes</strong> but also when <strong>serializing RDDs to disk</strong>. 在任何网络密集型场景下,我们推荐这么使用.</p>
<p>To register your own custom classes with Kryo, use the registerKryoClasses method.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br><span class="line">val sc = <span class="keyword">new</span> SparkContext(conf)</span><br></pre></td></tr></table></figure></p>
<p>If your objects are large, you may also need to increase the spark.kryoserializer.buffer ,这个值要大到能装下你要序列化的最大的对象.</p>
<p>Finally, if you don’t register your custom classes, Kryo will still work, 但是效率就没那么高了.</p>
<h3 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h3><p>内存优化有三个方面的考虑 : 对象所占用的内存，访问对象的消耗以及垃圾回收所占用的开销.默认情况下，Java 对象存取速度快，但可以很消耗空间,比内部 “raw” 数据的字段的消耗的 2-5 倍以及更多空间,主要的原因还是<strong>储存了额外信息或者编码导致的</strong>,这是语言决定了的,没办法去提升它.</p>
<h4 id="Memory-Management-Overview"><a href="#Memory-Management-Overview" class="headerlink" title="Memory Management Overview"></a>Memory Management Overview</h4><p>execution and storage. <strong>Execution memory</strong> refers to that used for computation in shuffles, joins, sorts and aggregations. <strong>storage memory</strong> refers to that used for caching (RDDs) and propagating internal data across the cluster.</p>
<p>In Spark, execution and storage 分享共同的内存区域 (M). When no execution memory is used, storage can acquire all the available memory.  Execution may evict storage if necessary, 但是Storage至少也需要R区域的内存,这就是threshold了. In other words, R describes a subregion within M where cached blocks are never evicted. Storage无法侵占Execution的内存,因为实现复杂.</p>
<p>我们可以看出几点</p>
<ul>
<li>没有使用chching的应用,所有内存都能用来Execution,避免不必要的<strong>磁盘溢出??</strong></li>
<li>使用了cache的应用,Storage有最小配额</li>
<li>这种方式提供了较好的性能,也不用问用户到底怎么分配内存.</li>
</ul>
<p>有两个参数可调,但是对大多数情况,他们不需要被调整.</p>
<ul>
<li>spark.memory.fraction: M大小占JVM堆内存大小的百分比. The rest of the space (40%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.</li>
<li>spark.memory.storageFraction: R占M的比例</li>
</ul>
<h4 id="估计Memory-Consumption"><a href="#估计Memory-Consumption" class="headerlink" title="估计Memory Consumption"></a>估计Memory Consumption</h4><p>你可以把数据集装入一个rdd里,然后把它cache,然后在Web UI可以看见这个RDD消耗了多少内存.</p>
<p>或者,To estimate the memory consumption of a particular object, use <strong>SizeEstimator’s estimate</strong> method This is useful for experimenting with different data layouts to trim memory usage, 对于确定一个广播变量需要在每个executtor堆上所需的内存也是实用的.</p>
<h4 id="Tuning-Data-Structures"><a href="#Tuning-Data-Structures" class="headerlink" title="Tuning Data Structures"></a>Tuning Data Structures</h4><ol>
<li>尽可能避免嵌套数据结构,那将包含很多小的对象和指针</li>
<li>RAM&lt;32g时,可开启-XX:+UseCompressedOops,让指针是4B而不是8B</li>
<li>对主键,采用数字ID或者枚举类型以替代String类型.</li>
<li>使用自己设计,或者比如fastutil类库来取代原始JAVA集合类</li>
</ol>
<h4 id="Serialized-RDD-Storage"><a href="#Serialized-RDD-Storage" class="headerlink" title="Serialized RDD Storage"></a>Serialized RDD Storage</h4><p>最简单的方式是以序列化形式储存rdd, , using the serialized StorageLevels in the RDD persistence API, such as <em>MEMORY_ONLY_SER</em>. Spark will then store each RDD partition as one large byte array. 唯一的缺陷就是access会慢一点因为需要反序列化,推荐使用Kryo来做.</p>
<h4 id="GC-Tuning"><a href="#GC-Tuning" class="headerlink" title="GC Tuning"></a>GC Tuning</h4><p>要记住,gc的开销与对象的数量是成正比的, so using data structures with fewer objects (e.g. an array of Ints instead of a LinkedList) greatly lowers this cost. 一个更好的方法是cache序列化的rdd,那么一个rdd就只会有一个对象:那就是序列化后的字符数组了.下面我们讨论如何控制为 RDD 分配空间以便减轻这种影响。</p>
<p>开启了参数-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,下一次发生gc以及持续的时间都会在worker的log下打印,注意不是在driver下.</p>
<p>The Young generation is further divided into three regions [Eden, Survivor1, Survivor2]</p>
<p>gc的简要描述是:A simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old enough or Survivor2 is full, it is moved to Old. Finally when Old is close to full, a full GC is invoked</p>
<p>我们的目的是确保只有长期存在的rdd在老年代内,并且年轻代足够放下那些短命rdd,这能尽可能减少full gc.</p>
<ul>
<li>首先检查,如果在task完成前,full gc出现多次,就该优化了.</li>
<li>如果有很多小集合,但没很多minor gc,那么可是尝试设置Eden大一些,You can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden is determined to be E, then you can set the size of the Young generation using the option -Xmn=4/3*E.</li>
<li>如果gc完老年代也快满尝试调低,spark.memory.fraction,宁愿少缓存一点数据,也不要降低task速度;老年代占heap的大小也应该超过spark.memory.fraction(这样可以使老年大足够大装下spark数据,减少gc).</li>
<li>甚至可以Try the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations where garbage collection is a bottleneck</li>
<li>如果task从HDFS读取数据,task使用的内存跟HDFS中读取的数据block大小有关.注意到,解压后的块儿大小可能有3个原本block那么大,.所以如果我们希望有4个任务的工作空间，HDFS块的大小是128 MB，我们可以估计Eden的大小是4 <em> 3 </em> 128MB。</li>
</ul>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ol>
<li>并行级别: Spark automatically sets the number of “map” tasks to run on each file <strong>according to its size</strong>; for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. 可以在spark.default.parallelism参数中设置,一般推荐每个CPU两到三个task.</li>
<li>Reduce的内存使用:Sometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory(Storage), but because the working set of one of your tasks, such as one of the reduce tasks in groupByKey, was too large(Execution). Spark’s shuffle operations (sortByKey, groupByKey, reduceByKey, join, etc) <strong>build a hash table within each task</strong> to perform the grouping, which can often be large. The simplest fix here is to <strong>increase the level of parallelism</strong>, so that each task’s input set is smaller</li>
<li>广播变量:减少task的大小和在cluster中启动job的开销. If your tasks use any large object from the driver program inside of them (e.g. a static lookup table), consider turning it into a broadcast variable. Spark会在master上打印每个task的序列化后大小, so you can look at that to decide whether your tasks are too large,一般来讲,超过20K的task就值得去优化了.</li>
<li>数据本地性:这对性能还算是个主要的影响因素,If data and the code that operates on it are together then computation tends to be fast. But if code and data are separated, one must move to the other. 一般来讲移动代码当然比移动数据集要快,现在有几种级别<ul>
<li>PROCESS_LOCAL:数据在运行代码的相同JVM中,这是最好的</li>
<li>NODE_LOCAL : data is on the same node. 比如在相同node上的HDFS中,或者另一个Executor中,This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li>
<li>NO_PREF data is <strong>accessed equally quickly</strong> from anywhere and has no locality preference??</li>
<li>RACK_LOCAL data在同rack的server上. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li>
<li>ANY : data不在同rack上<br>Spark 倾向于把任务都调度在best locality level,但不可能吧也;所以,当任何<strong>空闲Executor</strong>上都没有要处理的data,就要降低本地性级别.那么要么等待一个空闲的CPU在数据所在节点上来做.或者在别的地方开启task,这样数据需要运输.实际上,spark会等一会儿,超时之后,就放到把数据往别的空闲cpu上弄了. The <strong>wait timeout</strong> for fallback between each level can be configured individually or all together in one parameter; see the spark.locality parameters.</li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/17/hadoop中NameNode、DataNode、Secondary NameNode、JobTracker TaskTracker介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/17/hadoop中NameNode、DataNode、Secondary NameNode、JobTracker TaskTracker介绍/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-17T12:43:36+08:00">
                2017-04-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="hadoop中NameNode、DataNode、Secondary-NameNode、JobTracker-TaskTracker介绍"><a href="#hadoop中NameNode、DataNode、Secondary-NameNode、JobTracker-TaskTracker介绍" class="headerlink" title="hadoop中NameNode、DataNode、Secondary NameNode、JobTracker TaskTracker介绍"></a>hadoop中NameNode、DataNode、Secondary NameNode、JobTracker TaskTracker介绍</h1><p>#spark</p>
<p>NameNode、Secondary  NameNode、JobTracker运行在Master节点上，而在每个Slave节点上，部署一个DataNode和TaskTracker，以便 这个Slave服务器运行的数据处理程序能尽可能直接处理本机的数据。对Master节点需要特别说明的是，在小集群中，Secondary  NameNode可以属于某个从节点；在大型集群中，NameNode和JobTracker被分别部署在两台服务器上。</p>
<h4 id="namenode"><a href="#namenode" class="headerlink" title="namenode"></a>namenode</h4><p>Namenode 管理者文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。<strong>就相当于GFS里Master</strong></p>
<p>当Namenode出了问题,为了系统正常服务,有两种方式:<br>Secondary Namenode并不能被用作Namenode它的主要作用是定期的将Namespace镜像与操作日志文件(edit log)合并，以防止操作日志文件(edit log)变得过大。通常，Secondary Namenode 运行在一个单独的物理机上，因为合并操作需要占用大量的CPU时间以及和Namenode相当的内存。辅助Namenode保存着合并后的Namespace镜像的一个备份，万一哪天Namenode宕机了，这个备份就可以用上了。</p>
<p>但是辅助Namenode总是落后于主Namenode，所以在Namenode宕机时，数据丢失是不可避免的。在这种情况下，一般的，要结合第一种方式(备份):远程挂载的网络文件系统(NFS)中的Namenode的元数据文件来使用，把NFS中的Namenode元数据文件，拷贝到辅助Namenode，并把辅助Namenode作为主Namenode来运行。</p>
<h4 id="datanode"><a href="#datanode" class="headerlink" title="datanode"></a>datanode</h4><p>Datanode是文件系统的工作节点，他们根据客户端或者是namenode的调度存储和检索数据，并且定期向namenode发送他们所存储的块(block)的列表.当然,客户端交互的时候需要先访问NameNode取得地址.</p>
<h4 id="secondary-namenode"><a href="#secondary-namenode" class="headerlink" title="secondary namenode"></a>secondary namenode</h4><p>Secondary  NameNode不同于NameNode，它不接受或者记录任何实时的数据变化，但是，它会与NameNode进行通信，以便定期地保存HDFS元数据的 快照。由于NameNode是单点的，通过Secondary  NameNode的快照功能，可以将NameNode的宕机时间和数据损失降低到最小。同时，如果NameNode发生问题，Secondary  NameNode可以及时地作为备用NameNode使用<br><img src="/2017/04/17/hadoop中NameNode、DataNode、Secondary NameNode、JobTracker TaskTracker介绍/212842d1kgrsg1q5go66lk.jpg" alt=""><br>由于Edit log不断增长，在NameNode重启时，会造成长时间NameNode处于安全模式，不可用状态，是非常不符合Hadoop的设计初衷。所以要周期性合并Edit log，但是这个工作由NameNode来完成，会占用大量资源，这样就出现了Secondary NameNode，它可以进行image检查点的处理工作。步骤如下：</p>
<ol>
<li>Secondary NameNode请求NameNode进行edit log的滚动（即创建一个新的edit log），将新的编辑操作记录到新生成的edit log文件；</li>
<li>通过http get方式，读取NameNode上的fsimage和edits文件，到Secondary NameNode上；</li>
<li>读取fsimage到内存中，即加载fsimage到内存，然后执行edits中所有操作（类似OracleDG，应用redo log），并生成一个新的fsimage文件，即这个检查点被创建；</li>
<li>通过http post方式，将新的fsimage文件传送到NameNode；</li>
<li>NameNode使用新的fsimage替换原来的fsimage文件，让1创建的edits替代原来的edits文件；并且更新fsimage文件的检查点时间。</li>
</ol>
<p>整个处理过程完成。<br>Secondary NameNode的处理，是将fsimage和edites文件周期的合并，不会造成nameNode重启时造成长时间不可访问的情况。</p>
<h4 id="JT"><a href="#JT" class="headerlink" title="JT"></a>JT</h4><p>JobTracker后台程序用来连接应用程序与Hadoop。用户代码提交到集群以后，由JobTracker决定哪个文件将被处理，并且为不同的task分配节点(<strong>安排TaskTracker</strong>)。同时，它还监控所有的task</p>
<h4 id="TT"><a href="#TT" class="headerlink" title="TT"></a>TT</h4><p>TaskTracker与负责存储数据的DataNode相结合，TaskTrackers位于从节点，独立管理各自的task。<strong>每个TaskTracker负责独立执行具体的task</strong>，而 JobTracker负责分配task。虽然每个从节点仅有一个唯一的一个TaskTracker，但是每个TaskTracker可以产生多个java 虚拟机（JVM），用于并行处理多个map以及reduce任务。TaskTracker的一个重要职责就是<strong>与JobTracker交互</strong>。如果 JobTracker无法准时地获取TaskTracker提交的信息，JobTracker就判定TaskTracker已经崩溃，并将任务分配给其他 节点处理</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">PW</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PW</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
