<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="GDBT学习#推荐引擎/论文 这篇笔记里学习的步骤是:  普通决策树的构建算法 回归树CART Boosting Gradient Boosting Tree Spark GBDT demo.  网上摘的一些参考资料: [[决策树]] 普通决策树的构建算法 这是一个自上而下的递归的过程,递归返回的条件有三个  当前节点包含的样本都是同一类别,无需划分 当前属性集为空,或样本在属性集上取值相同,无法">
<meta property="og:type" content="article">
<meta property="og:title" content="PW&#39;s notes">
<meta property="og:url" content="http://yoursite.com/2018/01/15/GDBT学习/index.html">
<meta property="og:site_name" content="PW&#39;s notes">
<meta property="og:description" content="GDBT学习#推荐引擎/论文 这篇笔记里学习的步骤是:  普通决策树的构建算法 回归树CART Boosting Gradient Boosting Tree Spark GBDT demo.  网上摘的一些参考资料: [[决策树]] 普通决策树的构建算法 这是一个自上而下的递归的过程,递归返回的条件有三个  当前节点包含的样本都是同一类别,无需划分 当前属性集为空,或样本在属性集上取值相同,无法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/C9B45B5E-B3C2-40FE-8EB8-AEACF7F2689B.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/IMG_8237.JPG">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/63538259-CD5B-4A01-887D-9631A119B089.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/A9B4CF3A-E018-4DD5-90B4-05A275AC19BE.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/6F28C98D-4CBD-4CB9-BCFA-456D010F1340.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/707D8064-558F-4EE6-9B1B-5F2330ECFA10.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/8D94B1BA-07EC-4AEF-91B1-C7E4E8E234F4.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/FF9620BB-9B38-4C0C-B6A0-849D4F627402.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/C4C034BB-6BEB-4782-B6F7-E965391384B5.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/B6BCD192-89E4-4C63-BCF4-EAF1EE0A0977.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/36DAA479-56E5-42A7-906D-43505C58068B.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/F98833DD-65AA-4BBF-81C0-60C7960DC07F.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/EB5D7C33-C767-4FFE-812C-4384CBCEC2EB.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/32A865C8-EDBE-4BFC-992B-B190117743E6.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/63B6C514-EA50-437B-9B1F-8260FEA22437.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/42EB54CC-231C-45A8-8E4B-0741D0822F19.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/6D6563FE-3C16-45C8-A71C-B7C73C73D133.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/6154D156-3015-4F2E-AE2D-AA791DC59868.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/31610517-F485-4A89-A746-705B2396B00D.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/CD967556-0D48-4DEC-B861-9D13DAC97BC4.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/767C8C2D-7A52-4484-815F-6423B3102621.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/3D694F1D-C0BB-475B-82B9-66435D2E2E8F.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/F25F7812-7CF1-43AC-899C-E6051A58974E.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/35A5F5EF-6543-4FCD-926A-7404CCA588EA.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/5E2F365E-A2CB-4AF9-8C88-0ABD322D2456.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/91786C01-6306-4479-BDA5-126BC60FD363.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/IMG_9712_500x222.JPG">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/7BA219ED-0ABD-4CC7-A434-D694E47505EB.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/D9333131-D6BB-4381-9212-FDA1E245FBC3.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/8BEEFAEF-3AEF-4356-956F-D932D6A480A8.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/B60E3761-2327-4ACF-90EB-6E32F7749322.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/2AEB0BDD-C581-419B-A3B3-DD75BEAA76A9.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/EF9C7C59-42F9-4644-8931-5C5F40B7D9D9.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/2899D802-18DC-4FAF-863C-5EA7D531DDCA.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/27544280-9A45-4893-9186-97D5E1817B68.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/41F8ABFC-1B4A-4505-912C-ADFC5A417D68.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/1AF20DA0-BD85-4481-AD50-48F5727A7470.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/36CF82C0-E45B-434B-8453-E5221D3DA821.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/5574071C-DAB4-4DCC-B4C3-9D48528EDB6C.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/93578A52-E56A-4CC5-9B8C-B62AD8A23E9C.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/53FD1ED6-0A1E-44C1-9388-E4F8442E3230.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/86863016-EBD9-42B0-B9EB-7EBA93CC96B7.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/04C4B4D6-F8C6-4195-A86F-C051EB1B56BD.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/6A2D980E-935A-4A93-BD7F-13427B5A97BC.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/4A3C74B2-427E-485D-831B-A33512DC32AA.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/82C503A0-1A3A-45D1-AAE0-80E398975436.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/AA63EBF1-6916-4C6C-9805-56F4193C6848.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/FC4CAB04-0231-4217-A0C9-68CC242EEB71.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/8DE8536F-9DF6-4A19-B0C8-626B895330A2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/15/GDBT学习/AF2D790B-3DDE-4E87-92B5-B34BE7D88628.png">
<meta property="og:updated_time" content="2018-01-26T09:36:04.475Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PW&#39;s notes">
<meta name="twitter:description" content="GDBT学习#推荐引擎/论文 这篇笔记里学习的步骤是:  普通决策树的构建算法 回归树CART Boosting Gradient Boosting Tree Spark GBDT demo.  网上摘的一些参考资料: [[决策树]] 普通决策树的构建算法 这是一个自上而下的递归的过程,递归返回的条件有三个  当前节点包含的样本都是同一类别,无需划分 当前属性集为空,或样本在属性集上取值相同,无法">
<meta name="twitter:image" content="http://yoursite.com/2018/01/15/GDBT学习/C9B45B5E-B3C2-40FE-8EB8-AEACF7F2689B.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/01/15/GDBT学习/"/>





  <title> | PW's notes</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PW's notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/15/GDBT学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="PW">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PW's notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-15T19:05:15+08:00">
                2018-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="GDBT学习"><a href="#GDBT学习" class="headerlink" title="GDBT学习"></a>GDBT学习</h1><p>#推荐引擎/论文</p>
<p>这篇笔记里学习的步骤是:</p>
<ul>
<li>普通决策树的构建算法</li>
<li>回归树CART</li>
<li>Boosting</li>
<li>Gradient Boosting Tree</li>
<li>Spark GBDT demo.</li>
</ul>
<p>网上摘的一些参考资料: [[决策树]]</p>
<h2 id="普通决策树的构建算法"><a href="#普通决策树的构建算法" class="headerlink" title="普通决策树的构建算法"></a>普通决策树的构建算法</h2><hr>
<p><img src="/2018/01/15/GDBT学习/C9B45B5E-B3C2-40FE-8EB8-AEACF7F2689B.png" alt=""><br>这是一个自上而下的<strong>递归的过程</strong>,递归返回的条件有三个</p>
<ul>
<li>当前节点包含的样本都是同一类别,无需划分</li>
<li>当前属性集为空,或样本在属性集上取值相同,无法划分</li>
<li>当前节点包含的样本集合为空,不能划分</li>
</ul>
<p>用图解释一下:<br><img src="/2018/01/15/GDBT学习/IMG_8237.JPG" alt=""></p>
<p>然后就是如何选择<strong>最佳划分</strong>了.一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样 本尽可能属于同一类别，即结点的”<strong>纯度</strong>“ (purity)越来越高…</p>
<p>信息熵就是首先假设集合样本共有k类(2分类那就是k==2),然后:…<br><img src="/2018/01/15/GDBT学习/63538259-CD5B-4A01-887D-9631A119B089.png" alt=""><br>可知,信息熵越小,D的”<strong>纯度</strong>”越高,所以我们选择信息熵最小的划分,具体做法是<strong>对各种属性划分计算信息增益:</strong><br><img src="/2018/01/15/GDBT学习/A9B4CF3A-E018-4DD5-90B4-05A275AC19BE.png" alt=""><br>公式可以看出,∑项的信息熵越小,那么Gain就会越大,于是<strong>选择使得Gain最大的属性a</strong>作为最佳划分.</p>
<p>当然,，信息增益准则对可取值数目较多的属性有所偏好(<strong>取值多,划分后每个子集中样本可能就越少,越纯..)</strong>，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使 用信息增益，而是使用”增益率” (gain ratio) 来选择最优划分属性,定义如下:<br><img src="/2018/01/15/GDBT学习/6F28C98D-4CBD-4CB9-BCFA-456D010F1340.png" alt=""><br>可以看出,对IV(a),这是属性a的固有值,属性a可能取的数目越多,那么IV(a)的值一般就越大,可以降低取值数太多的影响.</p>
<p><strong>CART(回归树)</strong>使用<strong>基尼系数</strong>来选择划分属性,这也是为了度量<strong>数据集D的纯度</strong><br><img src="/2018/01/15/GDBT学习/707D8064-558F-4EE6-9B1B-5F2330ECFA10.png" alt=""><br>直观来说， Gini(D) <strong>反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率.因此， Gini(D) 越小，则数据集 D 的纯度越高</strong>.那么我们就对各种属性a的划分计算其<strong>基尼系数</strong><br><img src="/2018/01/15/GDBT学习/8D94B1BA-07EC-4AEF-91B1-C7E4E8E234F4.png" alt=""><br>选择划分时选择使得Geni_index最小的a即可.</p>
<h4 id="决策树的连续值与缺失值处理"><a href="#决策树的连续值与缺失值处理" class="headerlink" title="决策树的连续值与缺失值处理"></a>决策树的连续值与缺失值处理</h4><p>由于连续属性的可取值数目不再有限， 因 此，不能直接根据连续属性的可能取值来对结点进行划分.此时<strong>连续属性离散化</strong>技术可派上用场. 最简单的策略是<strong>采用二分法(bi-partition)对连续属性进行处理</strong>.<br>缺失值是值样本在有些attribute上没有取值的情况.<br>详细做法先不说明了.</p>
<h2 id="回归树CART"><a href="#回归树CART" class="headerlink" title="回归树CART"></a>回归树CART</h2><hr>
<p>from&lt;&lt;统计学习方法&gt;&gt;</p>
<p>之前的切分,属性有多少种取值就划分多少个枝子,一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以所以有观点认为这种切分方式过于迅速。另外一种方法是<strong>二元切分法</strong>，即每次把数据集切成两份。<strong>如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树</strong>。</p>
<p>之前的算法还只能把连续特征转换为离散型特征才能处理(ID3),而<strong>CART算法使用二元切分来处理连续型变量。对CART稍作修改就可以处理回归问题</strong>。CART决策树使用“基尼指数”来选择划分属性，基尼值是用来度量数据集的纯度。</p>
<p>CART同样由三个技术点:特征选择,树的生成,树的剪枝组成.得到的是<strong>给定X特征条件下输出变量Y的条件概率分布.</strong></p>
<p>算法分为两步</p>
<ol>
<li>生成树,要生成<strong>尽量大</strong>的树.</li>
<li>使用min{loss function}来对树进行剪枝.</li>
</ol>
<h4 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h4><p>树的生成过程中,对回归树使用MSE最小化,对分类树使用GeniIndex最小化准则,进行特征选择,生成二叉树.</p>
<p>对<strong>回归树</strong>,当划分都确定之后,使用<img src="/2018/01/15/GDBT学习/FF9620BB-9B38-4C0C-B6A0-849D4F627402.png" alt="">表示回归树对训练数据的预测误差,并且最小化这个来::求解每个节点上的输出值::,实际上,对于划分好的子节点,子节点Rm上的最优取值cm就是Rm上所有样本对应y的<strong>均值</strong></p>
<p>那么,如何划分呢? 若给定j和s,那么就可以定义左边和右边区域:<br><img src="/2018/01/15/GDBT学习/C4C034BB-6BEB-4782-B6F7-E965391384B5.png" alt=""><br>寻找最佳切分特征j和最佳切分点s的方法是求解下式:<br><img src="/2018/01/15/GDBT学习/B6BCD192-89E4-4C63-BCF4-EAF1EE0A0977.png" alt=""><br>步骤是,首先<strong>固定一个特征j</strong>(假设我们要按特征j进行划分),在此条件下,<strong>定一个j的切分点s</strong>,于是就得到左边区域和右边区域,并可知两个区域的值c1,c2…也就是说,<strong>一个s取值就对应了一对(c1,c2)</strong>.然后,使得上式[]中最小的s,就是<strong>最优的切分点s</strong>…进而,<strong>遍历所有j</strong>,就得到了<strong>最佳(j,s)对</strong>…<br>当然,由于s是连续的,我们也不能看清所有的s,我们就<strong>选择样本里出现过的s进行探究</strong>就好了.这也叫<strong>最小二乘回归树</strong></p>
<p>对分类树采用基尼系数进行划分,之前说过就不再说了,就是要注意这里是二叉树,你只能依据划分点把数据划分为2份然后计算基尼系数.实际问题里,就看你的取值是连续还是离散了…</p>
<p><strong>剪枝</strong>就不详细说了.</p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><hr>
<p>也就是<strong>集成学习</strong>.<br><img src="/2018/01/15/GDBT学习/36DAA479-56E5-42A7-906D-43505C58068B.png" alt=""><br>集成学习通过将多个学习器进行结合,常可获得比单一学习器显著优越的泛化性能.这对<strong>“弱学习器”</strong>(弱学习器常指繁华性能略优于随机猜测的学习器，例如在二分类问题上精度咯高于 50% 的分类器目)</p>
<p>在一般情况下,好的坏的混合,一般结果就比最好的要差一点,但是我们需要的是<strong>比任何单一学习器效果都好</strong>.要获得好的集成，个体学习器应”好而不同”,就是都需要有<strong>一定准确性</strong>,并且有一定<strong>差异性</strong>.(虽然这有点冲突)</p>
<p>根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类.</p>
<ol>
<li>个体学习器问存在<strong>强依赖关系</strong>、必须<strong>串行生成</strong>的序列化方法.(::Boosting::)</li>
<li>以及个体学习器间不存在强依赖关系、可同时生成的并行化方法(Bagging 和”随机森林” )</li>
</ol>
<p><strong>Boosting</strong> 是一族可将弱学习器提升为强学习器的算法.这族算法的工作机制类似:先从初始训练集训练出<strong>一个基学习器</strong>，再根据基学习器的表现<strong>对训练样本分布进行调整</strong>，使得先前基学习器<strong>做错的训练样本在后续受到更多关注</strong>， 然后<strong>基于调整后的样本分布来训练下一个基学习器</strong>; 如此重复进行，直至基学习器数目达到事先指定的值 T,最终将这T个基学习器进行<strong>加权结合</strong>.<br>Boosting 主要关注降低<strong>偏差</strong>，</p>
<p>下面简单以AdaBoost为例介绍一下.任务就是一个二分类任务{-1,+1}.<br><img src="/2018/01/15/GDBT学习/F98833DD-65AA-4BBF-81C0-60C7960DC07F.png" alt=""></p>
<ol>
<li>初始化最开始的分布D1(x)=1/m,也就是个multinoulli分布.</li>
<li>对训练轮数t<ol>
<li>使用基本分类器E来对样本D以数据分布Dt(x)来进行训练(数据分布Dt的含义是样本被用来训练的概率)</li>
<li>计算基分类器的误差率e</li>
<li>如果误差率e大于0.5就break算了</li>
<li>求α(t),含义是<strong>指数化损失函数</strong>对误差e的偏导数=0为0时的”基分类器线性组合方法H”</li>
<li>根据α(t)调整数据分布,如7..</li>
</ol>
</li>
<li>迭代t次以后输出sign(H)就是我们二分类所需要的分类函数….</li>
</ol>
<h2 id="Gradient-Boosting-Tree-GBDT"><a href="#Gradient-Boosting-Tree-GBDT" class="headerlink" title="Gradient Boosting Tree(GBDT)"></a>Gradient Boosting Tree(GBDT)</h2><hr>
<h3 id="Review-of-key-concepts-of-supervised-learning"><a href="#Review-of-key-concepts-of-supervised-learning" class="headerlink" title="Review of key concepts of supervised learning"></a>Review of key concepts of supervised learning</h3><p>不如先回顾一下::监督学习::的几个概念. 首先,定义需要使用的<strong>数学符号</strong>:<br><img src="/2018/01/15/GDBT学习/EB5D7C33-C767-4FFE-812C-4384CBCEC2EB.png" alt=""></p>
<p>supervised learning都有目标函数,其<strong>目标函数</strong>可拆分为:<br><img src="/2018/01/15/GDBT学习/32A865C8-EDBE-4BFC-992B-B190117743E6.png" alt=""><br>其中::损失函数::的自变量是Loss(y,y^),即<strong>真实值y</strong>和<strong>预测值y^</strong>的函数,这相当于<strong>bias偏差</strong>,下面是两种常用的Loss<br><img src="/2018/01/15/GDBT学习/63B6C514-EA50-437B-9B1F-8260FEA22437.png" alt=""><br>(logistic loss可分为两部分,看做yi=1,h()(预测值)=0和yi=0,h()=1的情况相加)<br><a href="http://blog.csdn.net/bitcarmanlee/article/details/51165444" target="_blank" rel="noopener">logistic回归详解(二）：损失函数（cost function）详解 - bitcarmanlee的博客 - CSDN博客</a><br>::正则项::就是跟参数w有关了,我们希望<strong>参数小(L2)</strong>或者<strong>少(L1)</strong>(<strong>相当于variance方差</strong>),下面是两种正则项:<br><img src="/2018/01/15/GDBT学习/42EB54CC-231C-45A8-8E4B-0741D0822F19.png" alt=""></p>
<p>代入使用的loss function和正则项,几个<strong>常见的回归的优化目标函数</strong>是:<br><img src="/2018/01/15/GDBT学习/6D6563FE-3C16-45C8-A71C-B7C73C73D133.png" alt=""></p>
<h3 id="Regression-Tree-and-Ensemble-What-are-we-Learning"><a href="#Regression-Tree-and-Ensemble-What-are-we-Learning" class="headerlink" title="Regression Tree and Ensemble (What are we Learning)"></a>Regression Tree and Ensemble (What are we Learning)</h3><p>一颗回归树做emsemble,就像下面这样,每棵树倒都是<strong>使用了所有训练instance</strong>.<br><img src="/2018/01/15/GDBT学习/6154D156-3015-4F2E-AE2D-AA791DC59868.png" alt=""><br>Why ensemble trees?</p>
<ol>
<li>和输入的数字规模无关,所以不需要特别注意的<strong>features normalization</strong>(将特征向量的每一维映射为均值为0,方差为1)</li>
<li>可以学习到特征间的<strong>高阶交互关系</strong></li>
<li>scalable,工业中好用.</li>
</ol>
<p>进入<strong>数学</strong>,假设我们有<strong>K</strong>棵树,于是对样本i的预测值y^i就是<br><img src="/2018/01/15/GDBT学习/31610517-F485-4A89-A746-705B2396B00D.png" alt=""><br>如果我们把每棵树的输出函数fk()看做参数,我们现在就是需要<strong>学习函数</strong>(而不是参数)</p>
<p>那我们如何定义emsemble tree目标函数呢?<br><img src="/2018/01/15/GDBT学习/CD967556-0D48-4DEC-B861-9D13DAC97BC4.png" alt=""><br>那么把正则化项定义为什么?树中节点数?L2 of leaf weights?…..</p>
<p>对决策树而言,经常都是<strong>启发式</strong>的,而非Objective…</p>
<ul>
<li>信息增益—&gt;损失 L()</li>
<li>剪枝—&gt;正则化by nodes</li>
<li>规定Max depth—&gt;constraint on the function space</li>
<li>Smoothing leaf values—&gt;L2 regulation on <em>leaf weights</em>.</li>
<li>回归树其实还可以做,分类,,回归,,Ranking…都可以.</li>
</ul>
<h3 id="Gradient-Boosting-How-do-we-Learn"><a href="#Gradient-Boosting-How-do-we-Learn" class="headerlink" title="Gradient Boosting (How do we Learn)"></a>Gradient Boosting (How do we Learn)</h3><p>现在已经介绍了我们要学什么模型,下面介绍如何学习模型,那就是::梯度提升::了.</p>
<p>对于上一小节介绍的目标函数,我们既然没有办法通过像SGD这样的方法去优化目标函数(寻找每一个f,因为他们是树,而不是一般向量)…于是我们使用additive training. 其中<img src="/2018/01/15/GDBT学习/767C8C2D-7A52-4484-815F-6423B3102621.png" alt="">表示<strong>训练t轮后,model对样本i的预测值</strong>,<strong>ft()表示第t轮的树</strong><br><img src="/2018/01/15/GDBT学习/3D694F1D-C0BB-475B-82B9-66435D2E2E8F.png" alt=""></p>
<p>由于在第t轮时有<br><img src="/2018/01/15/GDBT学习/F25F7812-7CF1-43AC-899C-E6051A58974E.png" alt="">…….(1)<br>于是第t轮时,把(1)代入Loss(y,y^),于是Loss()可以写成<br><img src="/2018/01/15/GDBT学习/35A5F5EF-6543-4FCD-926A-7404CCA588EA.png" alt="">………(2)<br>如果采用<strong>square loss</strong>,代入(2),那么Loss()就是<br><img src="/2018/01/15/GDBT学习/5E2F365E-A2CB-4AF9-8C88-0ABD322D2456.png" alt="">………(3)<br>把平方展开,化简为:<br><img src="/2018/01/15/GDBT学习/91786C01-6306-4479-BDA5-126BC60FD363.png" alt="">……(4)<br>甚至不用展开,可以化简为,<br><img src="/2018/01/15/GDBT学习/IMG_9712_500x222.JPG" alt="">……..(5)<br>注意到,(4)的椭圆圈圈里,或者(5)中的r的不就是::残差::吗?<br>所以说,在第t轮的时候,如果想要Loss极小,只需要(r-ft)够小,所以我们下一轮的树ft要<strong>拟合</strong>(接近)上一棵树的残差r.</p>
<p>按照这种情况,从&lt;&lt;统计学习方法里&gt;&gt;,一般采用square loss的GBDT构建算法是<br><img src="/2018/01/15/GDBT学习/7BA219ED-0ABD-4CC7-A434-D694E47505EB.png" alt=""><br>其中T是第m轮得到的单树,算法很好理解,也好实施.</p>
<p><strong>::但是::,我们想把模型抽象化,不想限于某种Loss…</strong></p>
<p>回忆一下泰勒公式<br><img src="/2018/01/15/GDBT学习/D9333131-D6BB-4381-9212-FDA1E245FBC3.png" alt=""></p>
<p>如果不采用square loss,在一般情况下,利用::泰勒展开::,可以把下面<strong>目标函数</strong><br><img src="/2018/01/15/GDBT学习/8BEEFAEF-3AEF-4356-956F-D932D6A480A8.png" alt=""><br>中的Loss()给展开(f()就是l(),x是<img src="/2018/01/15/GDBT学习/B60E3761-2327-4ACF-90EB-6E32F7749322.png" alt="">,δx是<img src="/2018/01/15/GDBT学习/2AEB0BDD-C581-419B-A3B3-DD75BEAA76A9.png" alt="">),进而写成:<br><img src="/2018/01/15/GDBT学习/EF9C7C59-42F9-4644-8931-5C5F40B7D9D9.png" alt=""><br>其中包含符号::gi::和::hi::(i表示instance)</p>
<ul>
<li>gi=<img src="/2018/01/15/GDBT学习/2899D802-18DC-4FAF-863C-5EA7D531DDCA.png" alt="">,即<strong>第t-1轮的损失</strong>对<strong>t-1轮预测值</strong>的导数.</li>
<li>hi=<img src="/2018/01/15/GDBT学习/27544280-9A45-4893-9186-97D5E1817B68.png" alt="">是<strong>二阶导数</strong></li>
<li>这不太好理解,那么,如果像上面采用square loss, 就有gi=2r,hi=2…</li>
</ul>
<p>最终,移除常数项,<strong>第t轮的优化目标函数</strong>就是<br><img src="/2018/01/15/GDBT学习/41F8ABFC-1B4A-4505-912C-ADFC5A417D68.png" alt=""><br>::只依赖于每个数据点的在误差函数上的一阶导数和二阶导数::</p>
<p><strong>::现在::,做好了转化,就让我们看看如何解这玩意儿.</strong></p>
<p><strong>首先</strong>,我们可以把回归树重新定义为两部分</p>
<ol>
<li>叶子节点的值组成的<strong>vector w</strong></li>
<li><strong>mapping function q()</strong>以把instance map到leaf, q(x)=j表示x实例落在了j号叶子上,进而,w(q(x))就表示x实例在树上的预测值,也就等于ft(xi).</li>
</ol>
<p><strong>其次</strong>,我们可以把一棵树的<strong>复杂度</strong>定义为:<br><img src="/2018/01/15/GDBT学习/1AF20DA0-BD85-4481-AD50-48F5727A7470.png" alt=""><br>其中包含两个常数,也就是说复杂度和<strong>叶子数T</strong>和<strong>叶子值w之和</strong>有关.可以用作<strong>正则化项</strong></p>
<p><strong>再者</strong>,把leaf j上的<strong>实例集合</strong>(Instance set)定义为<strong>Ij</strong><br><img src="/2018/01/15/GDBT学习/36CF82C0-E45B-434B-8453-E5221D3DA821.png" alt=""></p>
<p>我们之前的目标函数是按每个样本来∑,代入以上三点符号定义,我们按<strong>叶子</strong>重新group一下,把objective写成<br><img src="/2018/01/15/GDBT学习/5574071C-DAB4-4DCC-B4C3-9D48528EDB6C.png" alt=""><br><strong>This is sum of T independent quadratic functions,都是wj的二次函数</strong>,可以看到,只要叶子节点上的值取某些值,损失就能最小化,所以我们现在可以探究::叶子上的值到底怎么取::</p>
<p>我们发现,优化目标函数变成了一个<strong>二次函数的优化问题</strong>,二次函数的最值中学学过:<br><img src="/2018/01/15/GDBT学习/93578A52-E56A-4CC5-9B8C-B62AD8A23E9C.png" alt=""><br>若定义<br><img src="/2018/01/15/GDBT学习/53FD1ED6-0A1E-44C1-9388-E4F8442E3230.png" alt=""><br>那么优化目标函数可化简为:<br><img src="/2018/01/15/GDBT学习/86863016-EBD9-42B0-B9EB-7EBA93CC96B7.png" alt=""><br>于是目标函数是关于<strong>wj的二次函数</strong>,wj就是每个叶子的值</p>
<p>当树的结构structure q()已经确定,.于是按二次函数解法可以知道,在这个structure下最优的wj以及目标函数值就是:<br><img src="/2018/01/15/GDBT学习/04C4B4D6-F8C6-4195-A86F-C051EB1B56BD.png" alt=""><br>上图中<em>measure how good a structure is</em>就称为::structure score::,当然越小越好.<br><img src="/2018/01/15/GDBT学习/6A2D980E-935A-4A93-BD7F-13427B5A97BC.png" alt=""><br>::这样,由于Gj,和Hj(j代表leaf号)都是可以由上一轮的数据来计算,目标函数就解出来了.::</p>
<p>::现在已经知道如何优化第t轮目标函数了,下面依次来<strong>构建第t轮的一棵树</strong>::<br>下面的方法是<strong>正确</strong>的:</p>
<ol>
<li>穷举所有可能的structure q.</li>
<li>计算structure的structure score.</li>
<li>找到最佳的structure,然后叶子上的值取optimal leaf weight<br><img src="/2018/01/15/GDBT学习/4A3C74B2-427E-485D-831B-A33512DC32AA.png" alt=""></li>
</ol>
<ul>
<li>但是….穷举tree structure,这不太好吧…</li>
</ul>
<p>所以实操中,采用一种::贪心算法::来<strong>高效地</strong>learn the tree.</p>
<ol>
<li>从树的深度0开始.</li>
<li>从上至下,为叶子找最佳的split…然后可以计算这样split的增益<br><img src="/2018/01/15/GDBT学习/82C503A0-1A3A-45D1-AAE0-80E398975436.png" alt=""></li>
</ol>
<ul>
<li>那么,如何操作(2)这一步呢?<br>比如说我们要在年龄上选择划分xj&lt;a,(xj is age),那么<ol>
<li>首先把instance按年龄从左到右排序<br><img src="/2018/01/15/GDBT学习/AA63EBF1-6916-4C6C-9805-56F4193C6848.png" alt=""></li>
<li>然后从左到右扫描来插a点,没每个a点计算Gain,这就足够了..</li>
</ol>
</li>
</ul>
<p><strong>时间复杂度</strong>上,这样来产生一颗深度为k的树,有:<br><img src="/2018/01/15/GDBT学习/FC4CAB04-0231-4217-A0C9-68CC242EEB71.png" alt=""><br>其中nlogn是为instance排序的时间,然后d是feature个数,所以每一层需要dnlogn时间来确定,再加上我们需要k层,所以总时间为O(n d K logn)</p>
<p>对于<strong>离散值</strong>,我们也可以用之前的scoring fomula来计算score of a split based on categorical vatiables. 使用one-hot编码也可以.</p>
<p>对<strong>剪枝和正则化</strong>而言,当我们按下面公式寻找最佳split,找到的最佳gain<strong>可能会是负数</strong>……增益还负了,这肯定不好<br><img src="/2018/01/15/GDBT学习/8DE8536F-9DF6-4A19-B0C8-626B895330A2.png" alt=""><br>对此,有两种解决策略</p>
<ul>
<li><strong>Pre-stopping</strong>,当最佳split的gain为负的时候停止,但这样的split可能对以后的split是有好处的呢?</li>
<li><strong>Post-Prunning</strong>,生成到depth K,然后recursively prune all the leaf splits with negative gain.</li>
</ul>
<p>::现在已经知道如何构建第t轮的树了,那么可导出总的Boosted Tree Algorithm算法::</p>
<ol>
<li>每次循环加一颗新树</li>
<li>为每个instance计算(导数/梯度)gi,和hi,这只跟样本的真实值和上一轮的预测值有关,可以计算</li>
<li>使用(2)的数据来<strong>贪心地生成</strong>一颗树ft(x)</li>
<li>把这棵树加入到总的model:<br><img src="/2018/01/15/GDBT学习/AF2D790B-3DDE-4E87-92B5-B34BE7D88628.png" alt=""><br>e是常数,设置为0.1左右,称为步长….(This means we do not do full optimization in each step and reserve chance for future rounds, it helps prevent overfitting)</li>
</ol>
<h2 id="Spark-GBDT"><a href="#Spark-GBDT" class="headerlink" title="Spark GBDT"></a>Spark GBDT</h2><p>已经在本地运行了mllib里的GBDT例子,包括分类和回归,见代码即可.</p>
<p>另外,xgboost也提供了Spark支持,下面给几个链接说明xgboost怎么在spark上使用(scala)</p>
<ol>
<li><a href="https://github.com/dmlc/xgboost/tree/master/jvm-packages" target="_blank" rel="noopener">xgboost4j</a></li>
<li><a href="http://www.elenacuoco.com/2016/10/10/scala-spark-xgboost-classification/" target="_blank" rel="noopener">Spark And XGBoost using Scala</a></li>
<li><a href="https://www.qubole.com/blog/machine-learning-xgboost-qubole-spark-cluster/" target="_blank" rel="noopener">Machine Learning with XGBoost on Qubole Spark Cluster</a></li>
</ol>
<p><strong>交叉熵是衡量两个概率分布之间的差别的 越小则两个概率分布越相近 所以像逻辑回归这种输出概率的问题 采用交叉熵作为损失就能让训练去逼近训练集上代表的分布</strong></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/15/决策树/" rel="next" title="">
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/01/15/Learning To Rank/" rel="prev" title="">
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">PW</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GDBT学习"><span class="nav-number">1.</span> <span class="nav-text">GDBT学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#普通决策树的构建算法"><span class="nav-number">1.1.</span> <span class="nav-text">普通决策树的构建算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的连续值与缺失值处理"><span class="nav-number">1.1.0.1.</span> <span class="nav-text">决策树的连续值与缺失值处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归树CART"><span class="nav-number">1.2.</span> <span class="nav-text">回归树CART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#生成"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">生成</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">1.3.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Boosting-Tree-GBDT"><span class="nav-number">1.4.</span> <span class="nav-text">Gradient Boosting Tree(GBDT)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Review-of-key-concepts-of-supervised-learning"><span class="nav-number">1.4.1.</span> <span class="nav-text">Review of key concepts of supervised learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-Tree-and-Ensemble-What-are-we-Learning"><span class="nav-number">1.4.2.</span> <span class="nav-text">Regression Tree and Ensemble (What are we Learning)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Boosting-How-do-we-Learn"><span class="nav-number">1.4.3.</span> <span class="nav-text">Gradient Boosting (How do we Learn)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-GBDT"><span class="nav-number">1.5.</span> <span class="nav-text">Spark GBDT</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PW</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
